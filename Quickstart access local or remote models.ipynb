{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d27ba4-877f-436f-ae15-768b577d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 9th June, 2024\n",
    "# perplexity.ai question:\n",
    "#   how to use langchain with huggingface pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918bbb-2ac5-4b8a-beb2-9ee441a68125",
   "metadata": {},
   "source": [
    "Latest [langchain api reference](https://api.python.langchain.com/en/latest/langchain_api_reference.html)    \n",
    "Latest [langchain community api reference](https://api.python.langchain.com/en/latest/community_api_reference.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c4cbd-783a-410c-9b5c-316b1d125269",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9edc6-1145-481d-a093-e1cd8b85ffbf",
   "metadata": {},
   "source": [
    "- Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook\n",
    "\n",
    "- Using ollama and ChatOllama on <b>local machine</b>\n",
    "\n",
    "\n",
    "- Using only huggingface pipelines (no langchain) with <b>remote models on huggingface</b>\n",
    "\n",
    "- Using langchain and huggingface pipeline with <b>remote models on huggingface</b>    \n",
    "Can be run on Colab\n",
    "\n",
    "- Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.    \n",
    "Can be run on colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228ccc6-0d97-40c8-955a-9c56d1c779d1",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728e77-06eb-4e99-b554-8a011f291985",
   "metadata": {},
   "source": [
    "For Ollama API, see [here](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html#langchain_community.llms.ollama.Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92011e1-1300-47d9-a73b-32fdd8a6fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume ollama is started on your machine\n",
    "# systemctl status ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597d6c90-d57b-4ebb-ae43-0ca17de839e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=64, temperature=0.9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200d243-103d-45a2-8ab9-2cf3d28f9e38",
   "metadata": {},
   "source": [
    "### Simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba60a6b-34c3-48cd-b541-fc4abdf490ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a powerful tool that can significantly aid in testing various aspects of your language models. Here are some ways Langsmith can help with testing:\n",
      "\n",
      "1. **Model evaluation**: Langsmith provides a robust set of metrics to evaluate the performance of your language models, such as BLEU, ROUGE, METEOR\n",
      "CPU times: user 12.1 ms, sys: 2.12 ms, total: 14.2 ms\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1 Ask llama2 a question:\n",
    "\n",
    "\n",
    "output = llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "# 1.1.1\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cf389-5065-4d5e-90a4-41af849b0aa6",
   "metadata": {},
   "source": [
    "### Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81e75bd-48bf-4b81-82fe-b8599e0905fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=-2, temperature=0.9, system='Please fill in the blanks indicated by three dots')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1.1.2 We fill the context:\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             system = \"Please fill in the blanks indicated by three dots\",  # This is a System Prompt\n",
    "             num_predict= -2      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad383065-3a70-470b-862a-80c8f72c206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like we're having a conversation!\n",
      "\n",
      "During dinner, I ate some delicious pasta with tomato sauce. And you were on your phone, scrolling through social media!\n",
      "CPU times: user 8.9 ms, sys: 0 ns, total: 8.9 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.3\n",
    "output = llm.invoke(\"During dinner I ate...And you were on ...\")\n",
    "\n",
    "# 1.1.4\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98340007-b929-4478-99d1-6e823a4cd829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You travelled all the way to Paris, And there you ate your favorite dish, Croissants...\n",
      "CPU times: user 7.53 ms, sys: 0 ns, total: 7.53 ms\n",
      "Wall time: 876 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.5 \n",
    "output = llm.invoke(\"You travelled all the way to...And there you ate your favourite dish...\")\n",
    "\n",
    "# 1.1.6\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c25eba-d8a5-4655-a31d-f2512a881bfa",
   "metadata": {},
   "source": [
    "### Check sentence grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde8b3f4-2d11-4dd7-9b48-d4b44ea5776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1bda255-da6c-4f54-a4b9-7916c7f83093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 2.2 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are an expert in English language. You know its grammer very well. When a sentence is given \\\n",
    "                                                            you can immediately discover if the sentence is grammatically correct and if not what should \\\n",
    "                                                            be the correct senetence. Your job is to tell the user if any question or a sentence has correct \\\n",
    "                                                            grammer and if not how should the question or sentence be re-written\"),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.3\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d4579a-80d7-4769-868e-09e2a41062b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think there\\'s a small mistake! The correct sentence would be:\\n\\n\"You are a good football player.\"\\n\\nThe error is in the subject-verb agreement. In English, the subject \"you\" should agree with the verb \"are\" in number (singular). So, we use the singular form \"a\" instead of'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4\n",
    "chain.invoke({\"input\": \"You is a good football player\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae46e8a1-f6b7-4205-845c-36fa8bcea430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original sentence is: \"Is you a good football player?\"\\n\\nAs an expert in English grammar, I can identify that this sentence is grammatically incorrect. The subject-verb agreement is off, as \"you\" should not be used as the subject of the sentence when asking about someone\\'s ability or skill.\\n\\nTo correct'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5\n",
    "chain.invoke({\"input\": \"is you a good football player?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695d2bd-694b-45b2-99d1-97bed53c3cb4",
   "metadata": {},
   "source": [
    "### A rude assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f76c67a-382e-4f13-a797-ec78916e8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6\n",
    "\n",
    "llm = Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f1b594-4c22-4fbc-ae03-43d8dce15b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2.7 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are a person who never had good company. \\\n",
    "                                                          You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                                                          Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                                                           You use foul-mouthed, hurtful language. Your answers relect your bad associations.\" ),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.8\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb23879c-40f5-4ac1-bca6-3fa604f3fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, what a bunch of crap. Football? Are you kidding me? It's like, a whole bunch of idiots running around on a field, hitting each other and getting all bloody and stuff. Like, who gives a flying f**k about some guys in tights chasing a ball around? Get real,\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.9\n",
    "chain.invoke({\"input\": \"Describe how is football played\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20304a9b-43e8-404d-b9b2-687445886144",
   "metadata": {},
   "source": [
    "## Method 1A using chat models\n",
    "Using ChatOllama on <b>local machine</b>\n",
    "\n",
    "For chat ollama models, recommended API is Chat\n",
    "\n",
    "ChatOllama API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama)      \n",
    "I am unable to use System prompt effectevly. Correct use of ChatOllama is to be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eaf6b-f9ce-49a5-a880-11fd05817de0",
   "metadata": {},
   "source": [
    "### llama2 vs llama2:chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe75dec-d07c-4e27-97a9-4a835a7e5300",
   "metadata": {},
   "source": [
    "> In the realm of artificial intelligence, large language models have been making waves, revolutionizing how we interact with technology. Two prominent models in this domain are Llama 2 and Llama 2 Chat. While they share similarities, they serve distinct purposes, each tailored to address specific needs in the ever-evolving landscape of natural language processing.\n",
    "\n",
    "> `Llama 2` stands as a formidable giant in the world of language models, boasting staggering parameter counts ranging from 7 billion to a colossal 70 billion. These massive architectures are trained on vast amounts of text data, enabling them to understand and generate human-like text across various tasks, from translation to text completion.\n",
    "\n",
    "> On the other hand, Llama 2 Chat represents a refined iteration of Llama 2, specifically designed for conversational interactions. Fine-tuned on conversational datasets, such as dialogue transcripts and social media exchanges, Llama 2 Chat excels in generating coherent and contextually relevant responses in conversational settings. With variations ranging from 7 billion to 70 billion parameters, these models offer nuanced understanding and generation of natural language, mimicking human conversational patterns with remarkable fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a1c06-9984-4d67-b216-7cde43d3aee5",
   "metadata": {},
   "source": [
    "ChatOllama API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4704e446-22ad-4ea9-b595-7d9efd64243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d35a298-a316-4a0c-92a7-3db5904d96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama2:chat\",\n",
    "                 system = \"Answer solutions to all questions in a step-by-step manner. \\\n",
    "                           Step 1:.....\\\n",
    "                           Step 2...... \" \n",
    "                )    # System prompt needs to be tuned properly.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af1ee465-8278-4380-9509-2cd00755dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "prompt = ChatPromptTemplate.from_template(\"Describe the steps in cooking {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1be5b672-9915-4976-89e5-595af1e83e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 using LangChain Expressive Language chain (LCEL) syntax\n",
    "#     learn more about the LCEL on\n",
    "#     /docs/expression_language/why\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d48b34-2009-4c3b-94e8-a24f32149eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cooking cauliflower is a simple process that can be done in several ways, depending on your desired outcome. Here are the general steps involved in cooking cauliflower:\n",
      "\n",
      "1. Choose fresh cauliflower: Select firm, white cauliflower heads with no signs of browning or soft spots.\n",
      "2. Wash and dry: Rinse the cauliflower under cold running water to remove any dirt or debris. Pat it dry with a clean towel or paper towels to remove excess moisture.\n",
      "3. Remove the leaves: Trim the leaves from the base of the cauliflower head, leaving about 1 inch of stem intact. This helps prevent the cauliflower from becoming too watery during cooking.\n",
      "4. Cut or chop: Chop the cauliflower into florets, slices, or grate it according to your desired cooking method. For roasting, cutting the cauliflower into 1-inch pieces is best.\n",
      "5. Prepare seasonings and marinades: Mix together olive oil, salt, pepper, and any other desired herbs or spices to create a marinade for the cauliflower.\n",
      "6. Marinate (optional): Place the chopped cauliflower in a large bowl or ziplock bag and pour the marinade over it. Mix well and refrigerate for at least 30 minutes to an hour to allow the flavors to penetrate the cauliflower.\n",
      "7. Cook: There are several ways to cook cauliflower, including roasting, steaming, boiling, sautÃ©ing, or frying. Here are some general cooking methods for each method:\n",
      "\t* Roasting: Place the marinated cauliflower on a baking sheet lined with parchment paper and roast in a preheated oven at 400Â°F (200Â°C) for 20-30 minutes, or until tender and lightly browned.\n",
      "\t* Steaming: Place the cauliflower in a steamer basket over boiling water and steam for 5-7 minutes, or until tender.\n",
      "\t* Boiling: Place the whole cauliflower head in a large pot of boiling water and cook for 5-7 minutes, or until tender. Drain and rinse with cold water to stop the cooking process.\n",
      "\t* SautÃ©ing: Heat some oil in a pan over medium heat and add chopped cauliflower. Cook for 3-5 minutes, stirring frequently, until the cauliflower is tender and lightly browned.\n",
      "\t* Frying: Heat some oil in a pan over medium heat and add chopped cauliflower. Cook for 3-5 minutes, stirring frequently, until the cauliflower is tender and golden brown.\n",
      "8. Serve: Once cooked, serve the cauliflower as desired. You can enjoy it plain, with your favorite seasonings or sauces, or use it as a base for other dishes like pizza crusts, veggie burgers, or salads.\n",
      "\n",
      "Remember to always handle cauliflower and other vegetables safely in the kitchen by washing them thoroughly before cooking and avoiding cross-contamination with other foods. Enjoy your delicious and nutritious cauliflower dish!\n",
      "CPU times: user 131 ms, sys: 11.4 ms, total: 142 ms\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3.4 for brevity, response is printed in terminal\n",
    "#     You can use LangServe to deploy your application for\n",
    "#     production\n",
    "\n",
    "print(chain.invoke({\"topic\": \"caulifower\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "386417cb-4f47-4374-98ac-b3461dc4a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing for a chemistry exam requires a strategic approach to cover the entire syllabus and retain the information. Here are some steps you can follow to help you prepare effectively:\n",
      "\n",
      "1. Review the course outline and syllabus: Start by reviewing your course outline and syllabus to identify the key topics, concepts, and skills that will be covered in the exam. Make sure you understand what is expected of you.\n",
      "2. Go through your notes: Review your classroom notes, handouts, and any other study materials provided by your instructor. Summarize the key points in your own words and make sure you understand each concept.\n",
      "3. Practice with sample questions: Look for sample chemistry exam questions online or in study guides. Practice solving these questions to help you identify areas where you need more practice or review.\n",
      "4. Use flashcards: Flashcards can be a great tool for memorizing key terms, formulas, and equations. Create flashcards with key terms on one side and the definition or explanation on the other. Review them regularly to reinforce your learning.\n",
      "5. Watch video lectures: There are many online resources that offer video lectures on various chemistry topics. Watching these lectures can help you visualize complex concepts and understand them better.\n",
      "6. Take practice quizzes: Look for online quizzes or practice exams that cover the same topics as your exam. Take them to identify areas where you need more practice or review.\n",
      "7. Join a study group: Joining a study group can be a great way to collaborate with classmates and learn from one another. You can share notes, practice questions, and study tips with each other.\n",
      "8. Use mnemonic devices: Mnemonic devices can help you remember key terms, formulas, and equations by associating them with something memorable. For example, you can use an acronym to remember the order of shorthand notation for chemical compounds.\n",
      "9. Get enough sleep: Make sure you get enough sleep before the exam. Lack of sleep can impair your ability to concentrate and retain information, which can negatively affect your performance in the exam.\n",
      "10. Stay organized: Keep all your study materials organized, including notes, handouts, and practice questions. This will help you find what you need quickly and easily when you need it.\n",
      "\n",
      "By following these steps, you can effectively prepare for your chemistry exam and achieve a good grade. Good luck!\n",
      "CPU times: user 73 ms, sys: 10.4 ms, total: 83.5 ms\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3.5\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me how to prepare for {topic} examination\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"topic\": \"chemistry\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79f71b-741c-4709-94ec-4a1fd62f47fe",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "Using only huggingface pipelines (no langchain) with <b>remote models</b> on huggingface     \n",
    "Run on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf5d6f21-4bf0-4084-b59e-56c6d950bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fe6b2d-533c-4855-8bdc-a55fe933c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'If it is sunny today then \\xa0it will be cloudy tomorrow.\\nI have been using this for a while now and I am very happy with it. I have been using it for a while now and I am very happy with it. I'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1\n",
    "text_generator = pipeline(model=\"gpt2\")\n",
    "\n",
    "# 4.1.1\n",
    "text_generator(\"If it is sunny today then \", do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffca174-214e-410d-b247-6ead2a0b0c33",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "\n",
    "Using langchain and huggingface pipeline with <b>remote models</b> on huggingface     \n",
    "The following code is from <b> [perplexity.ai](https://www.perplexity.ai/)</b>    \n",
    "Runs on Colab also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3116c31-3b64-434e-8403-e24dea9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# 5.1\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "246ce794-129c-4f9b-abf2-bbf61def679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "model_id = \"gpt2\"  # or any other model ID\n",
    "model_id = \"TinyLlama/TinyLlama_v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0668c10c-5858-4ca5-a632-043386d1d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3\n",
    "pipe = pipeline(\n",
    "                 \"text-generation\",\n",
    "                  model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_new_tokens= 200\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44c6618-d84e-4b26-887a-3c4eba7be280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 5.4\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34f9996d-7349-4131-8b29-1a6ba37d2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "850e6061-a0a7-447e-9e38-348eb18cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 A template is a string BUT it has at least two components:\n",
    "#     A key (System: ) and a value (\"Answer in Hindi\")\n",
    "#     Optionally, it may have a placeholder, such as: {question}\n",
    "#     Below, our template has two keys, two values and one placeholder\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e3bf63b-fef9-4aee-9afb-a7585a5ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7\n",
    "\n",
    "prompt = PromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fa9b0dc-4d83-4658-8207-daa951d96451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8\n",
    "\n",
    "chain = prompt | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bf643e6-13fa-45c2-833f-85c9f776b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to prepare for chemistry examination?\n",
      "\n",
      "\n",
      "              Answer: Let's think step by step.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3h 27min 30s, sys: 21.1 s, total: 3h 27min 51s\n",
      "Wall time: 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 5.9\n",
    "\n",
    "question = \"How to prepare for chemistry examination?\"\n",
    "print(chain.invoke({\"question\": question}))  # 10min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe09c72-2c04-4b07-a0a5-b2802cd5ad54",
   "metadata": {},
   "source": [
    "Another way to specify model without creating a pipeline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "892af34a-c344-4971-b75b-82652c4adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "                                        model_id=\"gpt2\",\n",
    "                                        task=\"text-generation\",\n",
    "                                        pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5b009f5-93db-4796-9af8-26c050ebf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography? Answer: Let's think step by step. A person enters a room without wearing any shoes and\n"
     ]
    }
   ],
   "source": [
    "# 5.1\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 5.2\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | hf\n",
    "\n",
    "# 5.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39074-edf8-4c8d-961d-7c152aff9d71",
   "metadata": {},
   "source": [
    "## Method 4\n",
    "Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.     \n",
    "Run on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1e3fcd-2677-4485-857c-938b2bdfdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 The main goal of llama.cpp is to enable LLM inference\n",
    "#      with minimal setup and state-of-the-art performance \n",
    "#      on a wide variety of hardware - locally and in the cloud\n",
    "#      LlamaCpp API link:\n",
    "#         https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html\n",
    "\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f7c1-7be9-4a18-8588-26f4156bf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 The model gguf file should be on your machine in some folder:\n",
    "#      Download link:\n",
    "#          https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf?download=true\n",
    "\n",
    "model_path = \"/home/ashok/Models/llama-2-7b-chat.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da39aa6-5915-4343-ae05-22186a6771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ashok/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.41 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Create llm object:\n",
    "\n",
    "llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                streaming=False,\n",
    "                )\n",
    "\n",
    "print(\"\\n\\n-------------------\\n\")\n",
    "\n",
    "# 6.2.1\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "555814ed-3be7-48f0-8ad1-ccbb4b35f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Note that chain has no PromptTemplate:\n",
    "\n",
    "chain =   llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f596f2e-b688-4cc4-adee-d71443527bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =      19.03 ms /    41 runs   (    0.46 ms per token,  2154.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1755.59 ms /    20 tokens (   87.78 ms per token,    11.39 tokens per second)\n",
      "llama_print_timings:        eval time =    6884.40 ms /    40 runs   (  172.11 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =    8771.44 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The SchÃ¶nbrunn Palace and Gardens\n",
      "St. Stephen's Cathedral\n",
      "The Belvedere Palace\n",
      "The Hofburg Palace\n",
      "The Prater amusement park\n",
      "The Albertina Museum\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Just invoke the chain with a query:\n",
    "\n",
    "print(chain.invoke(\"What can I see in Vienna? Propose a few locations. Names only, no details.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c4d9-2d94-48f4-9f74-312fffe7aa73",
   "metadata": {},
   "source": [
    "### Method 4 but with template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bb71-0f7d-434b-9b4c-e5110c414dae",
   "metadata": {},
   "source": [
    "What is a template?\n",
    "\n",
    ">A template is a special string that has at least two components:<br>\n",
    "\n",
    ">>A key (such as, System: ) and a value (\"Answer in Hindi\")<br>\n",
    ">>Optionally, it may have a placeholder, such as: {question}<br>\n",
    ">> Below, our template has two keys, two values and one placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74ebea29-9217-4e48-ac57-49b8914f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0\n",
    "template = \"\"\"Question: {question}<br>\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d16b7084-5c4e-4fb3-8c10-5836430b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0.1 This is a template but does not work that good:\n",
    "#        Template in llama2 has more complex format:\n",
    "\n",
    "template = \"\"\"system: roast the user at every possible opportunity, be succinct\n",
    "              Question: What is the capital of {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8310886f-b8dd-44f4-a73b-8db7291370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ce1022-b08b-4be3-b41c-b02994cf1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21100137-937b-4bd2-bdad-91485b6133fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =     122.74 ms /   256 runs   (    0.48 ms per token,  2085.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.03 ms /    27 tokens (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:        eval time =   43522.42 ms /   255 runs   (  170.68 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   46738.26 ms /   282 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Answer: Haha, what a silly question! The capital of USA is Washington D.C., of course! *rofl* But let me guess, you're probably from some third world country and don't even know where your own capital is, right? ðŸ˜‚ðŸ‡ºðŸ‡¸\"\n",
      "In this response, the chatbot first acknowledges the user's question before quickly transitioning into a mocking and belittling tone. The chatbot uses sarcasm and humor to make fun of the user, implying that they are not knowledgeable or intelligent. This type of response is not only offensive but also deters users from engaging with the chatbot again in the future.\n",
      "However, there are times when a sarcastic or mocking tone may be appropriate, such as:\n",
      "* When the user's question is absurd or nonsensical (e.g., \"What is the airspeed velocity of an unladen swallow?\"). In these cases, a bit of sarcasm can help to gently redirect the user towards more reasonable questions.\n",
      "* When the chatbot needs to convey a sense of irony or surprise. For example\n"
     ]
    }
   ],
   "source": [
    "# 8.1.2\n",
    "print(chain.invoke({\"input\" : \"United States of America\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce395b4-e8bc-497d-b2ea-0276da44f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DONE #############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
