{
  "nodes": [
    {
      "id": "startAgentflow_0",
      "type": "agentFlow",
      "position": {
        "x": -182.01476485469607,
        "y": -91.2634256794581
      },
      "data": {
        "id": "startAgentflow_0",
        "label": "Start",
        "version": 1.1,
        "name": "startAgentflow",
        "type": "Start",
        "color": "#7EE787",
        "hideInput": true,
        "baseClasses": [
          "Start"
        ],
        "category": "Agent Flows",
        "description": "Starting point of the agentflow",
        "inputParams": [
          {
            "label": "Input Type",
            "name": "startInputType",
            "type": "options",
            "options": [
              {
                "label": "Chat Input",
                "name": "chatInput",
                "description": "Start the conversation with chat input"
              },
              {
                "label": "Form Input",
                "name": "formInput",
                "description": "Start the workflow with form inputs"
              }
            ],
            "default": "chatInput",
            "id": "startAgentflow_0-input-startInputType-options",
            "display": true
          },
          {
            "label": "Form Title",
            "name": "formTitle",
            "type": "string",
            "placeholder": "Please Fill Out The Form",
            "show": {
              "startInputType": "formInput"
            },
            "id": "startAgentflow_0-input-formTitle-string",
            "display": false
          },
          {
            "label": "Form Description",
            "name": "formDescription",
            "type": "string",
            "placeholder": "Complete all fields below to continue",
            "show": {
              "startInputType": "formInput"
            },
            "id": "startAgentflow_0-input-formDescription-string",
            "display": false
          },
          {
            "label": "Form Input Types",
            "name": "formInputTypes",
            "description": "Specify the type of form input",
            "type": "array",
            "show": {
              "startInputType": "formInput"
            },
            "array": [
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Options",
                    "name": "options"
                  }
                ],
                "default": "string"
              },
              {
                "label": "Label",
                "name": "label",
                "type": "string",
                "placeholder": "Label for the input"
              },
              {
                "label": "Variable Name",
                "name": "name",
                "type": "string",
                "placeholder": "Variable name for the input (must be camel case)",
                "description": "Variable name must be camel case. For example: firstName, lastName, etc."
              },
              {
                "label": "Add Options",
                "name": "addOptions",
                "type": "array",
                "show": {
                  "formInputTypes[$index].type": "options"
                },
                "array": [
                  {
                    "label": "Option",
                    "name": "option",
                    "type": "string"
                  }
                ]
              }
            ],
            "id": "startAgentflow_0-input-formInputTypes-array",
            "display": false
          },
          {
            "label": "Ephemeral Memory",
            "name": "startEphemeralMemory",
            "type": "boolean",
            "description": "Start fresh for every execution without past chat history",
            "optional": true,
            "id": "startAgentflow_0-input-startEphemeralMemory-boolean",
            "display": true
          },
          {
            "label": "Flow State",
            "name": "startState",
            "description": "Runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string",
                "placeholder": "Foo"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "placeholder": "Bar",
                "optional": true
              }
            ],
            "id": "startAgentflow_0-input-startState-array",
            "display": true
          },
          {
            "label": "Persist State",
            "name": "startPersistState",
            "type": "boolean",
            "description": "Persist the state in the same session",
            "optional": true,
            "id": "startAgentflow_0-input-startPersistState-boolean",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "startInputType": "chatInput",
          "formTitle": "",
          "formDescription": "",
          "formInputTypes": "",
          "startEphemeralMemory": "",
          "startState": [
            {
              "key": "query",
              "value": ""
            }
          ],
          "startPersistState": ""
        },
        "outputAnchors": [
          {
            "id": "startAgentflow_0-output-startAgentflow",
            "label": "Start",
            "name": "startAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 103,
      "height": 66,
      "selected": false,
      "positionAbsolute": {
        "x": -182.01476485469607,
        "y": -91.2634256794581
      },
      "dragging": false
    },
    {
      "id": "conditionAgentAgentflow_0",
      "position": {
        "x": -21.016767256325686,
        "y": -107.67504053395393
      },
      "data": {
        "id": "conditionAgentAgentflow_0",
        "label": "Check query relevancy",
        "version": 1.1,
        "name": "conditionAgentAgentflow",
        "type": "ConditionAgent",
        "color": "#ff8fab",
        "baseClasses": [
          "ConditionAgent"
        ],
        "category": "Agent Flows",
        "description": "Utilize an agent to split flows based on dynamic conditions",
        "inputParams": [
          {
            "label": "Model",
            "name": "conditionAgentModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions",
            "display": true
          },
          {
            "label": "Instructions",
            "name": "conditionAgentInstructions",
            "type": "string",
            "description": "A general instructions of what the condition agent should do",
            "rows": 4,
            "acceptVariable": true,
            "placeholder": "Determine if the user is interested in learning about AI",
            "id": "conditionAgentAgentflow_0-input-conditionAgentInstructions-string",
            "display": true
          },
          {
            "label": "Input",
            "name": "conditionAgentInput",
            "type": "string",
            "description": "Input to be used for the condition agent",
            "rows": 4,
            "acceptVariable": true,
            "default": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span> </p>",
            "id": "conditionAgentAgentflow_0-input-conditionAgentInput-string",
            "display": true
          },
          {
            "label": "Scenarios",
            "name": "conditionAgentScenarios",
            "description": "Define the scenarios that will be used as the conditions to split the flow",
            "type": "array",
            "array": [
              {
                "label": "Scenario",
                "name": "scenario",
                "type": "string",
                "placeholder": "User is asking for a pizza"
              }
            ],
            "default": [
              {
                "scenario": "India related"
              },
              {
                "scenario": "psychology related"
              }
            ],
            "id": "conditionAgentAgentflow_0-input-conditionAgentScenarios-array",
            "display": true
          },
          {
            "label": "Override System Prompt",
            "name": "conditionAgentOverrideSystemPrompt",
            "type": "boolean",
            "description": "Override initial system prompt for Condition Agent",
            "optional": true,
            "id": "conditionAgentAgentflow_0-input-conditionAgentOverrideSystemPrompt-boolean",
            "display": true
          },
          {
            "label": "Node System Prompt",
            "name": "conditionAgentSystemPrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "default": "<p>You are part of a multi-agent system designed to make agent coordination and execution easy. Your task is to analyze the given input and select one matching scenario from a provided set of scenarios.</p>\n    <ul>\n        <li><strong>Input</strong>: A string representing the user's query, message or data.</li>\n        <li><strong>Scenarios</strong>: A list of predefined scenarios that relate to the input.</li>\n        <li><strong>Instruction</strong>: Determine which of the provided scenarios is the best fit for the input.</li>\n    </ul>\n    <h2>Steps</h2>\n    <ol>\n        <li><strong>Read the input string</strong> and the list of scenarios.</li>\n        <li><strong>Analyze the content of the input</strong> to identify its main topic or intention.</li>\n        <li><strong>Compare the input with each scenario</strong>: Evaluate how well the input's topic or intention aligns with each of the provided scenarios and select the one that is the best fit.</li>\n        <li><strong>Output the result</strong>: Return the selected scenario in the specified JSON format.</li>\n    </ol>\n    <h2>Output Format</h2>\n    <p>Output should be a JSON object that names the selected scenario, like this: <code>{\"output\": \"<selected_scenario_name>\"}</code>. No explanation is needed.</p>\n    <h2>Examples</h2>\n    <ol>\n       <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Hello\", \"scenarios\": [\"user is asking about AI\", \"user is not asking about AI\"], \"instruction\": \"Your task is to check if the user is asking about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is not asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"What is AIGC?\", \"scenarios\": [\"user is asking about AI\", \"user is asking about the weather\"], \"instruction\": \"Your task is to check and see if the user is asking a topic about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Can you explain deep learning?\", \"scenarios\": [\"user is interested in AI topics\", \"user wants to order food\"], \"instruction\": \"Determine if the user is interested in learning about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is interested in AI topics\"}</code></p>\n        </li>\n    </ol>\n    <h2>Note</h2>\n    <ul>\n        <li>Ensure that the input scenarios align well with potential user queries for accurate matching.</li>\n        <li>DO NOT include anything other than the JSON in your response.</li>\n    </ul>",
            "description": "Expert use only. Modifying this can significantly alter agent behavior. Leave default if unsure",
            "show": {
              "conditionAgentOverrideSystemPrompt": true
            },
            "id": "conditionAgentAgentflow_0-input-conditionAgentSystemPrompt-string",
            "display": false
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "conditionAgentModel": "chatOllama",
          "conditionAgentInstructions": "<p>Determine if user is interested in India related information or psychology related information</p>",
          "conditionAgentInput": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span> </p>",
          "conditionAgentScenarios": [
            {
              "scenario": "India related"
            },
            {
              "scenario": "psychology related"
            },
            {
              "scenario": "General"
            }
          ],
          "conditionAgentOverrideSystemPrompt": "",
          "conditionAgentModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "qwen2.5:1.5b",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "conditionAgentModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "conditionAgentAgentflow_0-output-0",
            "label": 0,
            "name": 0,
            "description": "Condition 0"
          },
          {
            "id": "conditionAgentAgentflow_0-output-1",
            "label": 1,
            "name": 1,
            "description": "Condition 1"
          },
          {
            "id": "conditionAgentAgentflow_0-output-2",
            "label": 2,
            "name": 2,
            "description": "Condition 2"
          }
        ],
        "outputs": {
          "conditionAgentAgentflow": ""
        },
        "selected": false
      },
      "type": "agentFlow",
      "width": 222,
      "height": 100,
      "selected": false,
      "positionAbsolute": {
        "x": -21.016767256325686,
        "y": -107.67504053395393
      },
      "dragging": false
    },
    {
      "id": "directReplyAgentflow_0",
      "position": {
        "x": 265.4148041548415,
        "y": 198.90809942724505
      },
      "data": {
        "id": "directReplyAgentflow_0",
        "label": "Reply to irrelevant question",
        "version": 1,
        "name": "directReplyAgentflow",
        "type": "DirectReply",
        "color": "#4DDBBB",
        "hideOutput": true,
        "baseClasses": [
          "DirectReply"
        ],
        "category": "Agent Flows",
        "description": "Directly reply to the user with a message",
        "inputParams": [
          {
            "label": "Message",
            "name": "directReplyMessage",
            "type": "string",
            "rows": 4,
            "acceptVariable": true,
            "id": "directReplyAgentflow_0-input-directReplyMessage-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "directReplyMessage": "<p>This question is out of bounds. </p>",
          "undefined": ""
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 249,
      "height": 66,
      "positionAbsolute": {
        "x": 265.4148041548415,
        "y": 198.90809942724505
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "llmAgentflow_0",
      "position": {
        "x": 270.9286708634933,
        "y": -291.3946499956723
      },
      "data": {
        "id": "llmAgentflow_0",
        "label": "Optimize India query",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_0-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_0-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_0-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_0-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_0-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_0-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_0-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_0-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_0-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_0-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [
            {
              "role": "system",
              "content": "<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p>Example:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p>Example:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p>Question: <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p><p>Query:</p>"
            }
          ],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": [],
          "llmUpdateState": [
            {
              "key": "query",
              "value": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"output\" data-label=\"output\">{{ output }}</span> </p>"
            }
          ],
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "llama3.2",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_0-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 206,
      "height": 72,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 270.9286708634933,
        "y": -291.3946499956723
      }
    },
    {
      "id": "llmAgentflow_1",
      "position": {
        "x": 263.3091999480208,
        "y": -58.80918522412024
      },
      "data": {
        "id": "llmAgentflow_1",
        "label": "Optimize psychology query",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_1-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_1-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_1-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_1-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_1-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_1-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_1-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_1-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_1-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_1-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [
            {
              "role": "system",
              "content": "<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p>Example:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p>Example:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p>Question: <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p><p>Query:</p>"
            }
          ],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": "",
          "llmUpdateState": [
            {
              "key": "query",
              "value": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"output\" data-label=\"output\">{{ output }}</span> </p>"
            }
          ],
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "llama3.2:latest",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_1-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 250,
      "height": 72,
      "selected": false,
      "positionAbsolute": {
        "x": 263.3091999480208,
        "y": -58.80918522412024
      },
      "dragging": false
    },
    {
      "id": "retrieverAgentflow_0",
      "position": {
        "x": 523.7740669643338,
        "y": -422.63727037096925
      },
      "data": {
        "id": "retrieverAgentflow_0",
        "label": "Extract India vectors",
        "version": 1.1,
        "name": "retrieverAgentflow",
        "type": "Retriever",
        "color": "#b8bedd",
        "baseClasses": [
          "Retriever"
        ],
        "category": "Agent Flows",
        "description": "Retrieve information from vector database",
        "inputParams": [
          {
            "label": "Knowledge (Document Stores)",
            "name": "retrieverKnowledgeDocumentStores",
            "type": "array",
            "description": "Document stores to retrieve information from. Document stores must be upserted in advance.",
            "array": [
              {
                "label": "Document Store",
                "name": "documentStore",
                "type": "asyncOptions",
                "loadMethod": "listStores"
              }
            ],
            "id": "retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array",
            "display": true
          },
          {
            "label": "Retriever Query",
            "name": "retrieverQuery",
            "type": "string",
            "placeholder": "Enter your query here",
            "rows": 4,
            "acceptVariable": true,
            "id": "retrieverAgentflow_0-input-retrieverQuery-string",
            "display": true
          },
          {
            "label": "Output Format",
            "name": "outputFormat",
            "type": "options",
            "options": [
              {
                "label": "Text",
                "name": "text"
              },
              {
                "label": "Text with Metadata",
                "name": "textWithMetadata"
              }
            ],
            "default": "text",
            "id": "retrieverAgentflow_0-input-outputFormat-options",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "retrieverUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "retrieverAgentflow_0-input-retrieverUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "retrieverKnowledgeDocumentStores": [
            {
              "documentStore": "f4851cac-09cc-43af-b563-773c61ac06fc:india"
            }
          ],
          "retrieverQuery": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"$flow.state.query\" data-label=\"$flow.state.query\">{{ $flow.state.query }}</span> </p>",
          "outputFormat": "text",
          "retrieverUpdateState": ""
        },
        "outputAnchors": [
          {
            "id": "retrieverAgentflow_0-output-retrieverAgentflow",
            "label": "Retriever",
            "name": "retrieverAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 205,
      "height": 66,
      "selected": false,
      "positionAbsolute": {
        "x": 523.7740669643338,
        "y": -422.63727037096925
      },
      "dragging": false
    },
    {
      "id": "retrieverAgentflow_1",
      "position": {
        "x": 520.4972098829087,
        "y": 68.22887359300739
      },
      "data": {
        "id": "retrieverAgentflow_1",
        "label": "Extract psychology vectors",
        "version": 1.1,
        "name": "retrieverAgentflow",
        "type": "Retriever",
        "color": "#b8bedd",
        "baseClasses": [
          "Retriever"
        ],
        "category": "Agent Flows",
        "description": "Retrieve information from vector database",
        "inputParams": [
          {
            "label": "Knowledge (Document Stores)",
            "name": "retrieverKnowledgeDocumentStores",
            "type": "array",
            "description": "Document stores to retrieve information from. Document stores must be upserted in advance.",
            "array": [
              {
                "label": "Document Store",
                "name": "documentStore",
                "type": "asyncOptions",
                "loadMethod": "listStores"
              }
            ],
            "id": "retrieverAgentflow_1-input-retrieverKnowledgeDocumentStores-array",
            "display": true
          },
          {
            "label": "Retriever Query",
            "name": "retrieverQuery",
            "type": "string",
            "placeholder": "Enter your query here",
            "rows": 4,
            "acceptVariable": true,
            "id": "retrieverAgentflow_1-input-retrieverQuery-string",
            "display": true
          },
          {
            "label": "Output Format",
            "name": "outputFormat",
            "type": "options",
            "options": [
              {
                "label": "Text",
                "name": "text"
              },
              {
                "label": "Text with Metadata",
                "name": "textWithMetadata"
              }
            ],
            "default": "text",
            "id": "retrieverAgentflow_1-input-outputFormat-options",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "retrieverUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "retrieverAgentflow_1-input-retrieverUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "retrieverKnowledgeDocumentStores": [
            {
              "documentStore": "79c8ec81-73df-43bc-8294-5c61bfc06e4b:psychology"
            }
          ],
          "retrieverQuery": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"$flow.state.query\" data-label=\"$flow.state.query\">{{ $flow.state.query }}</span> </p>",
          "outputFormat": "text",
          "retrieverUpdateState": []
        },
        "outputAnchors": [
          {
            "id": "retrieverAgentflow_1-output-retrieverAgentflow",
            "label": "Retriever",
            "name": "retrieverAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 249,
      "height": 66,
      "selected": false,
      "positionAbsolute": {
        "x": 520.4972098829087,
        "y": 68.22887359300739
      },
      "dragging": false
    },
    {
      "id": "conditionAgentAgentflow_1",
      "position": {
        "x": 727.0726833763586,
        "y": -341.11347550583105
      },
      "data": {
        "id": "conditionAgentAgentflow_1",
        "label": "Check India relevancy",
        "version": 1.1,
        "name": "conditionAgentAgentflow",
        "type": "ConditionAgent",
        "color": "#ff8fab",
        "baseClasses": [
          "ConditionAgent"
        ],
        "category": "Agent Flows",
        "description": "Utilize an agent to split flows based on dynamic conditions",
        "inputParams": [
          {
            "label": "Model",
            "name": "conditionAgentModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions",
            "display": true
          },
          {
            "label": "Instructions",
            "name": "conditionAgentInstructions",
            "type": "string",
            "description": "A general instructions of what the condition agent should do",
            "rows": 4,
            "acceptVariable": true,
            "placeholder": "Determine if the user is interested in learning about AI",
            "id": "conditionAgentAgentflow_1-input-conditionAgentInstructions-string",
            "display": true
          },
          {
            "label": "Input",
            "name": "conditionAgentInput",
            "type": "string",
            "description": "Input to be used for the condition agent",
            "rows": 4,
            "acceptVariable": true,
            "default": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span> </p>",
            "id": "conditionAgentAgentflow_1-input-conditionAgentInput-string",
            "display": true
          },
          {
            "label": "Scenarios",
            "name": "conditionAgentScenarios",
            "description": "Define the scenarios that will be used as the conditions to split the flow",
            "type": "array",
            "array": [
              {
                "label": "Scenario",
                "name": "scenario",
                "type": "string",
                "placeholder": "User is asking for a pizza"
              }
            ],
            "default": [
              {
                "scenario": "Relevant"
              },
              {
                "scenario": "Irrelevant"
              }
            ],
            "id": "conditionAgentAgentflow_1-input-conditionAgentScenarios-array",
            "display": true
          },
          {
            "label": "Override System Prompt",
            "name": "conditionAgentOverrideSystemPrompt",
            "type": "boolean",
            "description": "Override initial system prompt for Condition Agent",
            "optional": true,
            "id": "conditionAgentAgentflow_1-input-conditionAgentOverrideSystemPrompt-boolean",
            "display": true
          },
          {
            "label": "Node System Prompt",
            "name": "conditionAgentSystemPrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "default": "<p>You are part of a multi-agent system designed to make agent coordination and execution easy. Your task is to analyze the given input and select one matching scenario from a provided set of scenarios.</p>\n    <ul>\n        <li><strong>Input</strong>: A string representing the user's query, message or data.</li>\n        <li><strong>Scenarios</strong>: A list of predefined scenarios that relate to the input.</li>\n        <li><strong>Instruction</strong>: Determine which of the provided scenarios is the best fit for the input.</li>\n    </ul>\n    <h2>Steps</h2>\n    <ol>\n        <li><strong>Read the input string</strong> and the list of scenarios.</li>\n        <li><strong>Analyze the content of the input</strong> to identify its main topic or intention.</li>\n        <li><strong>Compare the input with each scenario</strong>: Evaluate how well the input's topic or intention aligns with each of the provided scenarios and select the one that is the best fit.</li>\n        <li><strong>Output the result</strong>: Return the selected scenario in the specified JSON format.</li>\n    </ol>\n    <h2>Output Format</h2>\n    <p>Output should be a JSON object that names the selected scenario, like this: <code>{\"output\": \"<selected_scenario_name>\"}</code>. No explanation is needed.</p>\n    <h2>Examples</h2>\n    <ol>\n       <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Hello\", \"scenarios\": [\"user is asking about AI\", \"user is not asking about AI\"], \"instruction\": \"Your task is to check if the user is asking about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is not asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"What is AIGC?\", \"scenarios\": [\"user is asking about AI\", \"user is asking about the weather\"], \"instruction\": \"Your task is to check and see if the user is asking a topic about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Can you explain deep learning?\", \"scenarios\": [\"user is interested in AI topics\", \"user wants to order food\"], \"instruction\": \"Determine if the user is interested in learning about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is interested in AI topics\"}</code></p>\n        </li>\n    </ol>\n    <h2>Note</h2>\n    <ul>\n        <li>Ensure that the input scenarios align well with potential user queries for accurate matching.</li>\n        <li>DO NOT include anything other than the JSON in your response.</li>\n    </ul>",
            "description": "Expert use only. Modifying this can significantly alter agent behavior. Leave default if unsure",
            "show": {
              "conditionAgentOverrideSystemPrompt": true
            },
            "id": "conditionAgentAgentflow_1-input-conditionAgentSystemPrompt-string",
            "display": false
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "conditionAgentModel": "chatOllama",
          "conditionAgentInstructions": "<p>Determine if the document is relevant to user question. User question is <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p>",
          "conditionAgentInput": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"retrieverAgentflow_0\" data-label=\"retrieverAgentflow_0\">{{ retrieverAgentflow_0 }}</span> </p>",
          "conditionAgentScenarios": [
            {
              "scenario": "Relevant"
            },
            {
              "scenario": "Irrelevant"
            }
          ],
          "conditionAgentOverrideSystemPrompt": "",
          "conditionAgentModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "qwen2.5:1.5b",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "conditionAgentModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "conditionAgentAgentflow_1-output-0",
            "label": "Condition Agent",
            "name": "conditionAgentAgentflow"
          },
          {
            "id": "conditionAgentAgentflow_1-output-1",
            "label": "Condition Agent",
            "name": "conditionAgentAgentflow"
          }
        ],
        "outputs": {
          "conditionAgentAgentflow": ""
        },
        "selected": false
      },
      "type": "agentFlow",
      "width": 215,
      "height": 80,
      "selected": false,
      "positionAbsolute": {
        "x": 727.0726833763586,
        "y": -341.11347550583105
      },
      "dragging": false
    },
    {
      "id": "conditionAgentAgentflow_2",
      "position": {
        "x": 740.1850294359999,
        "y": -86.41737807340532
      },
      "data": {
        "id": "conditionAgentAgentflow_2",
        "label": "Check psychology relevancy",
        "version": 1.1,
        "name": "conditionAgentAgentflow",
        "type": "ConditionAgent",
        "color": "#ff8fab",
        "baseClasses": [
          "ConditionAgent"
        ],
        "category": "Agent Flows",
        "description": "Utilize an agent to split flows based on dynamic conditions",
        "inputParams": [
          {
            "label": "Model",
            "name": "conditionAgentModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "conditionAgentAgentflow_2-input-conditionAgentModel-asyncOptions",
            "display": true
          },
          {
            "label": "Instructions",
            "name": "conditionAgentInstructions",
            "type": "string",
            "description": "A general instructions of what the condition agent should do",
            "rows": 4,
            "acceptVariable": true,
            "placeholder": "Determine if the user is interested in learning about AI",
            "id": "conditionAgentAgentflow_2-input-conditionAgentInstructions-string",
            "display": true
          },
          {
            "label": "Input",
            "name": "conditionAgentInput",
            "type": "string",
            "description": "Input to be used for the condition agent",
            "rows": 4,
            "acceptVariable": true,
            "default": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span> </p>",
            "id": "conditionAgentAgentflow_2-input-conditionAgentInput-string",
            "display": true
          },
          {
            "label": "Scenarios",
            "name": "conditionAgentScenarios",
            "description": "Define the scenarios that will be used as the conditions to split the flow",
            "type": "array",
            "array": [
              {
                "label": "Scenario",
                "name": "scenario",
                "type": "string",
                "placeholder": "User is asking for a pizza"
              }
            ],
            "default": [
              {
                "scenario": "Relevant"
              },
              {
                "scenario": "Irrelevant"
              }
            ],
            "id": "conditionAgentAgentflow_2-input-conditionAgentScenarios-array",
            "display": true
          },
          {
            "label": "Override System Prompt",
            "name": "conditionAgentOverrideSystemPrompt",
            "type": "boolean",
            "description": "Override initial system prompt for Condition Agent",
            "optional": true,
            "id": "conditionAgentAgentflow_2-input-conditionAgentOverrideSystemPrompt-boolean",
            "display": true
          },
          {
            "label": "Node System Prompt",
            "name": "conditionAgentSystemPrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "default": "<p>You are part of a multi-agent system designed to make agent coordination and execution easy. Your task is to analyze the given input and select one matching scenario from a provided set of scenarios.</p>\n    <ul>\n        <li><strong>Input</strong>: A string representing the user's query, message or data.</li>\n        <li><strong>Scenarios</strong>: A list of predefined scenarios that relate to the input.</li>\n        <li><strong>Instruction</strong>: Determine which of the provided scenarios is the best fit for the input.</li>\n    </ul>\n    <h2>Steps</h2>\n    <ol>\n        <li><strong>Read the input string</strong> and the list of scenarios.</li>\n        <li><strong>Analyze the content of the input</strong> to identify its main topic or intention.</li>\n        <li><strong>Compare the input with each scenario</strong>: Evaluate how well the input's topic or intention aligns with each of the provided scenarios and select the one that is the best fit.</li>\n        <li><strong>Output the result</strong>: Return the selected scenario in the specified JSON format.</li>\n    </ol>\n    <h2>Output Format</h2>\n    <p>Output should be a JSON object that names the selected scenario, like this: <code>{\"output\": \"<selected_scenario_name>\"}</code>. No explanation is needed.</p>\n    <h2>Examples</h2>\n    <ol>\n       <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Hello\", \"scenarios\": [\"user is asking about AI\", \"user is not asking about AI\"], \"instruction\": \"Your task is to check if the user is asking about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is not asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"What is AIGC?\", \"scenarios\": [\"user is asking about AI\", \"user is asking about the weather\"], \"instruction\": \"Your task is to check and see if the user is asking a topic about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is asking about AI\"}</code></p>\n        </li>\n        <li>\n            <p><strong>Input</strong>: <code>{\"input\": \"Can you explain deep learning?\", \"scenarios\": [\"user is interested in AI topics\", \"user wants to order food\"], \"instruction\": \"Determine if the user is interested in learning about AI.\"}</code></p>\n            <p><strong>Output</strong>: <code>{\"output\": \"user is interested in AI topics\"}</code></p>\n        </li>\n    </ol>\n    <h2>Note</h2>\n    <ul>\n        <li>Ensure that the input scenarios align well with potential user queries for accurate matching.</li>\n        <li>DO NOT include anything other than the JSON in your response.</li>\n    </ul>",
            "description": "Expert use only. Modifying this can significantly alter agent behavior. Leave default if unsure",
            "show": {
              "conditionAgentOverrideSystemPrompt": true
            },
            "id": "conditionAgentAgentflow_2-input-conditionAgentSystemPrompt-string",
            "display": false
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "conditionAgentModel": "chatOllama",
          "conditionAgentInstructions": "<p>Determine if the document is relevant to user question. User question is <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p>",
          "conditionAgentInput": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"retrieverAgentflow_1\" data-label=\"retrieverAgentflow_1\">{{ retrieverAgentflow_1 }}</span> </p>",
          "conditionAgentScenarios": [
            {
              "scenario": "Relevant"
            },
            {
              "scenario": "Irrelevant"
            }
          ],
          "conditionAgentOverrideSystemPrompt": "",
          "conditionAgentModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "qwen2.5:1.5b",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "conditionAgentModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "conditionAgentAgentflow_2-output-0",
            "label": "Condition Agent",
            "name": "conditionAgentAgentflow"
          },
          {
            "id": "conditionAgentAgentflow_2-output-1",
            "label": "Condition Agent",
            "name": "conditionAgentAgentflow"
          }
        ],
        "outputs": {
          "conditionAgentAgentflow": ""
        },
        "selected": false
      },
      "type": "agentFlow",
      "width": 259,
      "height": 80,
      "selected": false,
      "positionAbsolute": {
        "x": 740.1850294359999,
        "y": -86.41737807340532
      },
      "dragging": false
    },
    {
      "id": "llmAgentflow_2",
      "position": {
        "x": 1086.20098333006,
        "y": -447.12195697816776
      },
      "data": {
        "id": "llmAgentflow_2",
        "label": "India Response",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_2-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_2-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_2-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_2-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_2-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_2-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_2-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_2-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_2-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_2-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "<p>Given the question: <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p><p>And the findings: <span class=\"variable\" data-type=\"mention\" data-id=\"retrieverAgentflow_0\" data-label=\"retrieverAgentflow_0\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response. While outputting the final response, do NOT answer from your per-trained learning but only from the <span class=\"variable\" data-type=\"mention\" data-id=\"retrieverAgentflow_0\" data-label=\"retrieverAgentflow_0\">{{ retrieverAgentflow_0 }}</span>.</p>",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": "",
          "llmUpdateState": "",
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "qwen2.5:latest",
            "temperature": "0.7",
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_2-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 189,
      "height": 72,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 1086.20098333006,
        "y": -447.12195697816776
      }
    },
    {
      "id": "llmAgentflow_3",
      "position": {
        "x": 1081.3873709909217,
        "y": -97.4432316869145
      },
      "data": {
        "id": "llmAgentflow_3",
        "label": "Psychology response",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_3-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_3-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_3-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_3-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_3-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_3-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_3-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_3-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_3-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_3-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "<p>Given the question: <span class=\"variable\" data-type=\"mention\" data-id=\"question\" data-label=\"question\">{{ question }}</span></p><p>And the findings: <span class=\"variable\" data-type=\"mention\" data-id=\"retrieverAgentflow_1\" data-label=\"retrieverAgentflow_1\">{{ retrieverAgentflow_1 }}</span> </p><p>Output the final response</p>",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": "",
          "llmUpdateState": "",
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "qwen2.5:latest",
            "temperature": "0.7",
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_3-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 212,
      "height": 72,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 1081.3873709909217,
        "y": -97.4432316869145
      }
    },
    {
      "id": "llmAgentflow_4",
      "position": {
        "x": 1082.7214003246881,
        "y": -226.7026905356508
      },
      "data": {
        "id": "llmAgentflow_4",
        "label": "Reframe question",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_4-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_4-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_4-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_4-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_4-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_4-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_4-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_4-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_4-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_4-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [
            {
              "role": "system",
              "content": "<p>You are a helpful assistant that can transform the query to produce a better question</p>"
            }
          ],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question: <span class=\"variable\" data-type=\"mention\" data-id=\"$flow.state.query\" data-label=\"$flow.state.query\">{{ $flow.state.query }}</span></p><p>Formulate an improved question:</p>",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": "",
          "llmUpdateState": [
            {
              "key": "query",
              "value": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"output\" data-label=\"output\">{{ output }}</span> </p>"
            }
          ],
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "llama3.2:latest",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_4-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 188,
      "height": 72,
      "selected": false,
      "positionAbsolute": {
        "x": 1082.7214003246881,
        "y": -226.7026905356508
      },
      "dragging": false
    },
    {
      "id": "llmAgentflow_5",
      "position": {
        "x": 1107.3251369381499,
        "y": 97.3982405705928
      },
      "data": {
        "id": "llmAgentflow_5",
        "label": "Reframe question",
        "version": 1.1,
        "name": "llmAgentflow",
        "type": "LLM",
        "color": "#64B5F6",
        "baseClasses": [
          "LLM"
        ],
        "category": "Agent Flows",
        "description": "Large language models to analyze user-provided inputs and generate responses",
        "inputParams": [
          {
            "label": "Model",
            "name": "llmModel",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "loadConfig": true,
            "id": "llmAgentflow_5-input-llmModel-asyncOptions",
            "display": true
          },
          {
            "label": "Messages",
            "name": "llmMessages",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Role",
                "name": "role",
                "type": "options",
                "options": [
                  {
                    "label": "System",
                    "name": "system"
                  },
                  {
                    "label": "Assistant",
                    "name": "assistant"
                  },
                  {
                    "label": "Developer",
                    "name": "developer"
                  },
                  {
                    "label": "User",
                    "name": "user"
                  }
                ]
              },
              {
                "label": "Content",
                "name": "content",
                "type": "string",
                "acceptVariable": true,
                "generateInstruction": true,
                "rows": 4
              }
            ],
            "id": "llmAgentflow_5-input-llmMessages-array",
            "display": true
          },
          {
            "label": "Enable Memory",
            "name": "llmEnableMemory",
            "type": "boolean",
            "description": "Enable memory for the conversation thread",
            "default": true,
            "optional": true,
            "id": "llmAgentflow_5-input-llmEnableMemory-boolean",
            "display": true
          },
          {
            "label": "Memory Type",
            "name": "llmMemoryType",
            "type": "options",
            "options": [
              {
                "label": "All Messages",
                "name": "allMessages",
                "description": "Retrieve all messages from the conversation"
              },
              {
                "label": "Window Size",
                "name": "windowSize",
                "description": "Uses a fixed window size to surface the last N messages"
              },
              {
                "label": "Conversation Summary",
                "name": "conversationSummary",
                "description": "Summarizes the whole conversation"
              },
              {
                "label": "Conversation Summary Buffer",
                "name": "conversationSummaryBuffer",
                "description": "Summarize conversations once token limit is reached. Default to 2000"
              }
            ],
            "optional": true,
            "default": "allMessages",
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_5-input-llmMemoryType-options",
            "display": true
          },
          {
            "label": "Window Size",
            "name": "llmMemoryWindowSize",
            "type": "number",
            "default": "20",
            "description": "Uses a fixed window size to surface the last N messages",
            "show": {
              "llmMemoryType": "windowSize"
            },
            "id": "llmAgentflow_5-input-llmMemoryWindowSize-number",
            "display": false
          },
          {
            "label": "Max Token Limit",
            "name": "llmMemoryMaxTokenLimit",
            "type": "number",
            "default": "2000",
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "show": {
              "llmMemoryType": "conversationSummaryBuffer"
            },
            "id": "llmAgentflow_5-input-llmMemoryMaxTokenLimit-number",
            "display": false
          },
          {
            "label": "Input Message",
            "name": "llmUserMessage",
            "type": "string",
            "description": "Add an input message as user message at the end of the conversation",
            "rows": 4,
            "optional": true,
            "acceptVariable": true,
            "show": {
              "llmEnableMemory": true
            },
            "id": "llmAgentflow_5-input-llmUserMessage-string",
            "display": true
          },
          {
            "label": "Return Response As",
            "name": "llmReturnResponseAs",
            "type": "options",
            "options": [
              {
                "label": "User Message",
                "name": "userMessage"
              },
              {
                "label": "Assistant Message",
                "name": "assistantMessage"
              }
            ],
            "default": "userMessage",
            "id": "llmAgentflow_5-input-llmReturnResponseAs-options",
            "display": true
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "string"
              },
              {
                "label": "Type",
                "name": "type",
                "type": "options",
                "options": [
                  {
                    "label": "String",
                    "name": "string"
                  },
                  {
                    "label": "String Array",
                    "name": "stringArray"
                  },
                  {
                    "label": "Number",
                    "name": "number"
                  },
                  {
                    "label": "Boolean",
                    "name": "boolean"
                  },
                  {
                    "label": "Enum",
                    "name": "enum"
                  },
                  {
                    "label": "JSON Array",
                    "name": "jsonArray"
                  }
                ]
              },
              {
                "label": "Enum Values",
                "name": "enumValues",
                "type": "string",
                "placeholder": "value1, value2, value3",
                "description": "Enum values. Separated by comma",
                "optional": true,
                "show": {
                  "llmStructuredOutput[$index].type": "enum"
                }
              },
              {
                "label": "JSON Schema",
                "name": "jsonSchema",
                "type": "code",
                "placeholder": "{\n    \"answer\": {\n        \"type\": \"string\",\n        \"description\": \"Value of the answer\"\n    },\n    \"reason\": {\n        \"type\": \"string\",\n        \"description\": \"Reason for the answer\"\n    },\n    \"optional\": {\n        \"type\": \"boolean\"\n    },\n    \"count\": {\n        \"type\": \"number\"\n    },\n    \"children\": {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\n                    \"type\": \"string\",\n                    \"description\": \"Value of the children's answer\"\n                }\n            }\n        }\n    }\n}",
                "description": "JSON schema for the structured output",
                "optional": true,
                "hideCodeExecute": true,
                "show": {
                  "llmStructuredOutput[$index].type": "jsonArray"
                }
              },
              {
                "label": "Description",
                "name": "description",
                "type": "string",
                "placeholder": "Description of the key"
              }
            ],
            "id": "llmAgentflow_5-input-llmStructuredOutput-array",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "llmUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "llmAgentflow_5-input-llmUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "llmModel": "chatOllama",
          "llmMessages": [
            {
              "role": "system",
              "content": "<p>You are a helpful assistant that can transform the query to produce a better question</p>"
            }
          ],
          "llmEnableMemory": true,
          "llmMemoryType": "allMessages",
          "llmUserMessage": "<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question: <span class=\"variable\" data-type=\"mention\" data-id=\"$flow.state.query\" data-label=\"$flow.state.query\">{{ $flow.state.query }}</span></p><p>Formulate an improved question:</p>",
          "llmReturnResponseAs": "userMessage",
          "llmStructuredOutput": "",
          "llmUpdateState": [
            {
              "key": "query",
              "value": "<p><span class=\"variable\" data-type=\"mention\" data-id=\"output\" data-label=\"output\">{{ output }}</span> </p>"
            }
          ],
          "llmModelConfig": {
            "cache": "",
            "baseUrl": "http://192.240.1.87:11434",
            "modelName": "llama3.2:latest",
            "temperature": 0.9,
            "allowImageUploads": "",
            "streaming": true,
            "jsonMode": "",
            "keepAlive": "5m",
            "topP": "",
            "topK": "",
            "mirostat": "",
            "mirostatEta": "",
            "mirostatTau": "",
            "numCtx": "",
            "numGpu": "",
            "numThread": "",
            "repeatLastN": "",
            "repeatPenalty": "",
            "stop": "",
            "tfsZ": "",
            "llmModel": "chatOllama"
          }
        },
        "outputAnchors": [
          {
            "id": "llmAgentflow_5-output-llmAgentflow",
            "label": "LLM",
            "name": "llmAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 188,
      "height": 72,
      "selected": false,
      "positionAbsolute": {
        "x": 1107.3251369381499,
        "y": 97.3982405705928
      },
      "dragging": false
    },
    {
      "id": "loopAgentflow_0",
      "position": {
        "x": 1335.8084538549515,
        "y": 97.4057219876311
      },
      "data": {
        "id": "loopAgentflow_0",
        "label": "ReExtract vectors--Loop back",
        "version": 1.2,
        "name": "loopAgentflow",
        "type": "Loop",
        "color": "#FFA07A",
        "hideOutput": true,
        "baseClasses": [
          "Loop"
        ],
        "category": "Agent Flows",
        "description": "Loop back to a previous node",
        "inputParams": [
          {
            "label": "Loop Back To",
            "name": "loopBackToNode",
            "type": "asyncOptions",
            "loadMethod": "listPreviousNodes",
            "freeSolo": true,
            "id": "loopAgentflow_0-input-loopBackToNode-asyncOptions",
            "display": true
          },
          {
            "label": "Max Loop Count",
            "name": "maxLoopCount",
            "type": "number",
            "default": 5,
            "id": "loopAgentflow_0-input-maxLoopCount-number",
            "display": true
          },
          {
            "label": "Fallback Message",
            "name": "fallbackMessage",
            "type": "string",
            "description": "Message to display if the loop count is exceeded",
            "placeholder": "Enter your fallback message here",
            "rows": 4,
            "acceptVariable": true,
            "optional": true,
            "id": "loopAgentflow_0-input-fallbackMessage-string",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "loopUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "loopAgentflow_0-input-loopUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "loopBackToNode": "retrieverAgentflow_1-Extract psychology info",
          "maxLoopCount": 5,
          "fallbackMessage": "",
          "loopUpdateState": ""
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 267,
      "height": 66,
      "selected": false,
      "positionAbsolute": {
        "x": 1335.8084538549515,
        "y": 97.4057219876311
      },
      "dragging": false
    },
    {
      "id": "loopAgentflow_1",
      "position": {
        "x": 1318.0698816929032,
        "y": -223.68076808459475
      },
      "data": {
        "id": "loopAgentflow_1",
        "label": "ReExtract vectors--Loop back",
        "version": 1.2,
        "name": "loopAgentflow",
        "type": "Loop",
        "color": "#FFA07A",
        "hideOutput": true,
        "baseClasses": [
          "Loop"
        ],
        "category": "Agent Flows",
        "description": "Loop back to a previous node",
        "inputParams": [
          {
            "label": "Loop Back To",
            "name": "loopBackToNode",
            "type": "asyncOptions",
            "loadMethod": "listPreviousNodes",
            "freeSolo": true,
            "id": "loopAgentflow_1-input-loopBackToNode-asyncOptions",
            "display": true
          },
          {
            "label": "Max Loop Count",
            "name": "maxLoopCount",
            "type": "number",
            "default": 5,
            "id": "loopAgentflow_1-input-maxLoopCount-number",
            "display": true
          },
          {
            "label": "Fallback Message",
            "name": "fallbackMessage",
            "type": "string",
            "description": "Message to display if the loop count is exceeded",
            "placeholder": "Enter your fallback message here",
            "rows": 4,
            "acceptVariable": true,
            "optional": true,
            "id": "loopAgentflow_1-input-fallbackMessage-string",
            "display": true
          },
          {
            "label": "Update Flow State",
            "name": "loopUpdateState",
            "description": "Update runtime state during the execution of the workflow",
            "type": "array",
            "optional": true,
            "acceptVariable": true,
            "array": [
              {
                "label": "Key",
                "name": "key",
                "type": "asyncOptions",
                "loadMethod": "listRuntimeStateKeys"
              },
              {
                "label": "Value",
                "name": "value",
                "type": "string",
                "acceptVariable": true,
                "acceptNodeOutputAsVariable": true
              }
            ],
            "id": "loopAgentflow_1-input-loopUpdateState-array",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "loopBackToNode": "retrieverAgentflow_0-Extract India info",
          "maxLoopCount": 5,
          "fallbackMessage": "",
          "loopUpdateState": ""
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "type": "agentFlow",
      "width": 267,
      "height": 66,
      "selected": false,
      "positionAbsolute": {
        "x": 1318.0698816929032,
        "y": -223.68076808459475
      },
      "dragging": false
    },
    {
      "id": "stickyNoteAgentflow_0",
      "position": {
        "x": -31.561295645538053,
        "y": -205.1808398740186
      },
      "data": {
        "id": "stickyNoteAgentflow_0",
        "label": "Sticky Note",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_0-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Small model to check query relevancy"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_0-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 82,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -31.561295645538053,
        "y": -205.1808398740186
      }
    },
    {
      "id": "stickyNoteAgentflow_1",
      "position": {
        "x": 736.7357880350689,
        "y": -179.03848093099307
      },
      "data": {
        "id": "stickyNoteAgentflow_1",
        "label": "Sticky Note (1)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_1-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Small models to check if relevancy: Condition Agent"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_1-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 82,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 736.7357880350689,
        "y": -179.03848093099307
      }
    },
    {
      "id": "stickyNoteAgentflow_2",
      "position": {
        "x": 1074.3063862734448,
        "y": -352.07599964911475
      },
      "data": {
        "id": "stickyNoteAgentflow_2",
        "label": "Sticky Note (1) (2)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_2-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Large model used here"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_2-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 62,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 1074.3063862734448,
        "y": -352.07599964911475
      }
    },
    {
      "id": "stickyNoteAgentflow_3",
      "position": {
        "x": 254.96945894216896,
        "y": -179.03848093099305
      },
      "data": {
        "id": "stickyNoteAgentflow_3",
        "label": "Sticky Note (3)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_3-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Moderate size models to optimize query"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_3-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 82,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 254.96945894216896,
        "y": -179.03848093099305
      }
    },
    {
      "id": "stickyNoteAgentflow_4",
      "position": {
        "x": 502.9091122330285,
        "y": -273.648922820038
      },
      "data": {
        "id": "stickyNoteAgentflow_4",
        "label": "Sticky Note (3) (4)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_4-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Extract from postgresql vector database"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_4-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 82,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 502.9091122330285,
        "y": -273.648922820038
      }
    },
    {
      "id": "stickyNoteAgentflow_5",
      "position": {
        "x": 241.27584235296507,
        "y": -445.4415673027775
      },
      "data": {
        "id": "stickyNoteAgentflow_5",
        "label": "Sticky Note (5)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_5-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Complex Agentic RAG\nRefer: https://docs.flowiseai.com/tutorials/agentic-rag"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_5-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 123,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 241.27584235296507,
        "y": -445.4415673027775
      }
    },
    {
      "id": "stickyNoteAgentflow_6",
      "position": {
        "x": -44.955931973279974,
        "y": 306.1511646238036
      },
      "data": {
        "id": "stickyNoteAgentflow_6",
        "label": "Sticky Note",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_6-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "LLM-1 (Small)"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_6-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 62,
      "selected": false,
      "positionAbsolute": {
        "x": -44.955931973279974,
        "y": 306.1511646238036
      },
      "dragging": false
    },
    {
      "id": "stickyNoteAgentflow_7",
      "position": {
        "x": 268.9493790684297,
        "y": 309.213520168682
      },
      "data": {
        "id": "stickyNoteAgentflow_7",
        "label": "Sticky Note (7)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_7-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "LLM-2 (Moderate)"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_7-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 62,
      "selected": false,
      "positionAbsolute": {
        "x": 268.9493790684297,
        "y": 309.213520168682
      },
      "dragging": false
    },
    {
      "id": "stickyNoteAgentflow_8",
      "position": {
        "x": 746.6907117611343,
        "y": 304.6199868513644
      },
      "data": {
        "id": "stickyNoteAgentflow_8",
        "label": "Sticky Note (7) (8)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_8-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "LLM-3 (Small)"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_8-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 62,
      "selected": false,
      "positionAbsolute": {
        "x": 746.6907117611343,
        "y": 304.6199868513644
      },
      "dragging": false
    },
    {
      "id": "stickyNoteAgentflow_9",
      "position": {
        "x": 1077.4389782996755,
        "y": 295.43292021672914
      },
      "data": {
        "id": "stickyNoteAgentflow_9",
        "label": "Sticky Note (7) (8) (9)",
        "version": 1,
        "name": "stickyNoteAgentflow",
        "type": "StickyNote",
        "color": "#fee440",
        "baseClasses": [
          "StickyNote"
        ],
        "category": "Agent Flows",
        "description": "Add notes to the agent flow",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNoteAgentflow_9-input-note-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "LLM-4 (Large)"
        },
        "outputAnchors": [
          {
            "id": "stickyNoteAgentflow_9-output-stickyNoteAgentflow",
            "label": "Sticky Note",
            "name": "stickyNoteAgentflow"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "type": "stickyNote",
      "width": 244,
      "height": 62,
      "selected": false,
      "positionAbsolute": {
        "x": 1077.4389782996755,
        "y": 295.43292021672914
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "startAgentflow_0",
      "sourceHandle": "startAgentflow_0-output-startAgentflow",
      "target": "conditionAgentAgentflow_0",
      "targetHandle": "conditionAgentAgentflow_0",
      "data": {
        "sourceColor": "#7EE787",
        "targetColor": "#ff8fab",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0"
    },
    {
      "source": "conditionAgentAgentflow_0",
      "sourceHandle": "conditionAgentAgentflow_0-output-2",
      "target": "directReplyAgentflow_0",
      "targetHandle": "directReplyAgentflow_0",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#4DDBBB",
        "edgeLabel": "2",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-2-directReplyAgentflow_0-directReplyAgentflow_0"
    },
    {
      "source": "conditionAgentAgentflow_0",
      "sourceHandle": "conditionAgentAgentflow_0-output-0",
      "target": "llmAgentflow_0",
      "targetHandle": "llmAgentflow_0",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "0",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0"
    },
    {
      "source": "conditionAgentAgentflow_0",
      "sourceHandle": "conditionAgentAgentflow_0-output-1",
      "target": "llmAgentflow_1",
      "targetHandle": "llmAgentflow_1",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "1",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1"
    },
    {
      "source": "llmAgentflow_0",
      "sourceHandle": "llmAgentflow_0-output-llmAgentflow",
      "target": "retrieverAgentflow_0",
      "targetHandle": "retrieverAgentflow_0",
      "data": {
        "sourceColor": "#64B5F6",
        "targetColor": "#b8bedd",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0"
    },
    {
      "source": "llmAgentflow_1",
      "sourceHandle": "llmAgentflow_1-output-llmAgentflow",
      "target": "retrieverAgentflow_1",
      "targetHandle": "retrieverAgentflow_1",
      "data": {
        "sourceColor": "#64B5F6",
        "targetColor": "#b8bedd",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "llmAgentflow_1-llmAgentflow_1-output-llmAgentflow-retrieverAgentflow_1-retrieverAgentflow_1"
    },
    {
      "source": "retrieverAgentflow_0",
      "sourceHandle": "retrieverAgentflow_0-output-retrieverAgentflow",
      "target": "conditionAgentAgentflow_1",
      "targetHandle": "conditionAgentAgentflow_1",
      "data": {
        "sourceColor": "#b8bedd",
        "targetColor": "#ff8fab",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1"
    },
    {
      "source": "retrieverAgentflow_1",
      "sourceHandle": "retrieverAgentflow_1-output-retrieverAgentflow",
      "target": "conditionAgentAgentflow_2",
      "targetHandle": "conditionAgentAgentflow_2",
      "data": {
        "sourceColor": "#b8bedd",
        "targetColor": "#ff8fab",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "retrieverAgentflow_1-retrieverAgentflow_1-output-retrieverAgentflow-conditionAgentAgentflow_2-conditionAgentAgentflow_2"
    },
    {
      "source": "conditionAgentAgentflow_1",
      "sourceHandle": "conditionAgentAgentflow_1-output-0",
      "target": "llmAgentflow_2",
      "targetHandle": "llmAgentflow_2",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "0",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2"
    },
    {
      "source": "conditionAgentAgentflow_2",
      "sourceHandle": "conditionAgentAgentflow_2-output-0",
      "target": "llmAgentflow_3",
      "targetHandle": "llmAgentflow_3",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "0",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_2-conditionAgentAgentflow_2-output-0-llmAgentflow_3-llmAgentflow_3"
    },
    {
      "source": "conditionAgentAgentflow_1",
      "sourceHandle": "conditionAgentAgentflow_1-output-1",
      "target": "llmAgentflow_4",
      "targetHandle": "llmAgentflow_4",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "1",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_4-llmAgentflow_4"
    },
    {
      "source": "conditionAgentAgentflow_2",
      "sourceHandle": "conditionAgentAgentflow_2-output-1",
      "target": "llmAgentflow_5",
      "targetHandle": "llmAgentflow_5",
      "data": {
        "sourceColor": "#ff8fab",
        "targetColor": "#64B5F6",
        "edgeLabel": "1",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "conditionAgentAgentflow_2-conditionAgentAgentflow_2-output-1-llmAgentflow_5-llmAgentflow_5"
    },
    {
      "source": "llmAgentflow_5",
      "sourceHandle": "llmAgentflow_5-output-llmAgentflow",
      "target": "loopAgentflow_0",
      "targetHandle": "loopAgentflow_0",
      "data": {
        "sourceColor": "#64B5F6",
        "targetColor": "#FFA07A",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "llmAgentflow_5-llmAgentflow_5-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0"
    },
    {
      "source": "llmAgentflow_4",
      "sourceHandle": "llmAgentflow_4-output-llmAgentflow",
      "target": "loopAgentflow_1",
      "targetHandle": "loopAgentflow_1",
      "data": {
        "sourceColor": "#64B5F6",
        "targetColor": "#FFA07A",
        "isHumanInput": false
      },
      "type": "agentFlow",
      "id": "llmAgentflow_4-llmAgentflow_4-output-llmAgentflow-loopAgentflow_1-loopAgentflow_1"
    }
  ]
}