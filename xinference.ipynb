{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqXnfEZ6aeBNPrvw8co2tw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/LLMs/blob/main/xinference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with Xinference"
      ],
      "metadata": {
        "id": "9Rqag09B71kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Xinference"
      ],
      "metadata": {
        "id": "ICRvWA2q76cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">*X*orbits *Inference* (Xinference) is an open-source platform to streamline the operation and integration of a wide array of AI models. With Xinference, you’re empowered to run inference using any open-source LLMs, embedding models, and multimodal models either in the cloud or on your own premises, and create robust AI-driven applications."
      ],
      "metadata": {
        "id": "6jxyFa6s8IfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM vs llama.cpp"
      ],
      "metadata": {
        "id": "-wJBPOSn97tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " vLLM is faster but it’s gpu only, so you’ll need quite a bit of vram for larger models.\n",
        "\n",
        "vLLM also does not support variable bit quantization. So no q3,q5,q6s."
      ],
      "metadata": {
        "id": "Gr9oDNdK9_hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are LLM Model Formats?"
      ],
      "metadata": {
        "id": "ORrKxBCV_9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">LLMs are massive, and to make them practical for deployment, especially on consumer hardware, their size needs to be reduced. Model formats are techniques for achieving this reduction, primarily through quantization. Quantization involves converting the model's weights and sometimes its activations from higher precision (like 32-bit or 16-bit) to lower bit formats (like 8-bit or 4-bit), which significantly decreases memory usage and computational load.\n"
      ],
      "metadata": {
        "id": "3koOBQQn_7Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why different formats are needed"
      ],
      "metadata": {
        "id": "64CDHtrkBiIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The original, full-precision versions of LLMs are massive, consisting of billions of parameters. This makes them expensive and difficult to run on standard consumer hardware, such as personal computers or mobile phones. Model formats solve this problem by:\n",
        "\n",
        ">>Reducing model size: Quantization, a core technique in many formats, maps the model's weights from high-precision formats (like 32-bit floats) to lower-precision formats (like 4-bit integers). This significantly shrinks the model file.\n",
        "\n",
        ">>Speeding up inference: A smaller, more efficient model requires less memory and can run faster, especially on devices with limited resources.\n",
        "\n",
        "\n",
        ">>Enabling broad compatibility: Some formats are designed to run on specific hardware, such as CPUs, while others are optimized for GPUs.\n",
        "\n",
        "\n",
        ">>Prominent LLM model formats:\n",
        "\n",
        ">>GGUF (GPT-Generated Unified Format)\n",
        "\n",
        ">>>Purpose: The GGUF format is optimized for running models on consumer-grade hardware, particularly CPUs, while also supporting GPU offloading for acceleration.\n",
        "\n",
        ">>>Design: It is a single-file, binary format that stores all model parameters, metadata, and tokenizer information. This makes it unambiguous, easily shareable, and fast to load.\n",
        "\n",
        ">>>Quantization: The format supports multiple quantization levels, including 2-bit, 4-bit, and 8-bit, giving users control over the trade-off between model size, speed, and accuracy.\n",
        "GPTQ (General Pre-trained Transformer Quantization)\n",
        "\n",
        ">>>Purpose: GPTQ is a technique for one-shot post-training quantization that heavily focuses on accelerating inference on GPUs.\n",
        "\n",
        ">>>Design: It quantizes the model layer-by-layer to a very low bit-width (often 4-bit) while minimizing the loss of accuracy. It achieves this by updating other weights in a group to compensate for the quantization error of one weight.\n",
        "\n",
        ">>>Implementation: During inference, the 4-bit integer weights are dequantized to higher precision on the fly within the GPU's memory. This is much faster than loading larger, full-precision weights.\n",
        "\n",
        "\n",
        ">>AWQ (Activation-aware Weight Quantization)\n",
        "\n",
        ">>>Purpose: AWQ is another quantization method that focuses on compressing models to 4-bits with minimal performance degradation, especially for GPU inference.\n",
        "\n",
        ">>>Design: The key insight of AWQ is that not all model weights are equally important for performance. It identifies and protects only the most \"salient\" (important) weights from quantization based on the distribution of their activation values. The remaining weights are quantized to a lower precision.\n",
        "\n",
        ">>>Benefit: By focusing the quantization on less critical weights, AWQ significantly reduces model size while retaining accuracy.\n",
        "\n",
        "\n",
        ">>ONNX (Open Neural Network Exchange)\n",
        "\n",
        ">>>Purpose: ONNX is an open standard designed to promote interoperability between different machine learning frameworks, such as PyTorch and TensorFlow.\n",
        "\n",
        ">>>Design: It provides a universal format for representing a model's computational graph and parameters. This allows a model trained in one framework to be converted to ONNX and then deployed for inference in another.\n",
        "\n",
        ">>>Application: While not a quantization method itself, models saved in ONNX format can be used with compatible runtimes that offer hardware-specific optimizations.\n"
      ],
      "metadata": {
        "id": "JL1JW7uVBda-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install xinference\n",
        "Ref [here](https://inference.readthedocs.io/en/latest/getting_started/installation.html#installation)"
      ],
      "metadata": {
        "id": "lyCuNorEC-TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to python environment and use pip to install, as:\n",
        "\n",
        "`./activate_langchain_venv.sh `   \n",
        "`pip install \"xinference[all]\"`   \n",
        "\n"
      ],
      "metadata": {
        "id": "G0khiHolDAoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base URL:   \n",
        "xinference base url is:     \n",
        "\n",
        "`http://192.240.4.102:9997`    "
      ],
      "metadata": {
        "id": "jo6rlHEBN_WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xinference expts\n",
        "See [here](https://inference.readthedocs.io/en/v1.4.1/getting_started/using_xinference.html)"
      ],
      "metadata": {
        "id": "6OkF-hMmJ3H4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run xinference locally, as:\n",
        "\n",
        "`./activate_langchain_venv.sh`    \n",
        "`xinference-local --host 0.0.0.0 --port 9997`"
      ],
      "metadata": {
        "id": "zsE8JkVkKjI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List all models of a certain type that are available to launch in Xinference:     \n",
        "\n",
        "`xinference registrations -t LLM`    \n",
        "\n",
        "the currently running models in Xinference:     \n",
        "\n",
        "`xinference list`    \n",
        "\n",
        "When you no longer need a model that is currently running, you can remove it in the following way to free up the resources it occupies:     \n",
        "\n",
        "`xinference terminate --model-uid \"qwen2.5-instruct\"`\n",
        "\n"
      ],
      "metadata": {
        "id": "8FUQJbHyLMC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backends"
      ],
      "metadata": {
        "id": "LKWl5PizMXfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Xinference supports multiple backends for different models. After the user specifies the model, xinference will automatically select the appropriate backend. Some supported backends are:     \n",
        ">>> llama.cpp     \n",
        ">>> vLLM    \n",
        ">>>>It is a fast and easy-to-use library for LLM inference and serving. It uses only GPU    \n",
        "\n",
        ">>> Auto NGL    \n",
        ">>> Transformers: vllm     \n",
        ">>>>Transformers supports the inference of most state-of-art models. It is the default backend for models in PyTorch format.    \n",
        "\n",
        ">>> SGLang    \n",
        ">>> MLX     "
      ],
      "metadata": {
        "id": "YXMmkS2hMbGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mqJ5oLa37ut6"
      },
      "outputs": [],
      "source": [
        "###########"
      ]
    }
  ]
}