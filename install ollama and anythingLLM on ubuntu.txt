# Last amended: 2nd Sep, 2025
# Installing anythingLLM with ollama on ubuntu 22.04
# It works with a GUI
# Ref: YouTube Video: https://www.youtube.com/watch?v=IJYC6zf86lU


What is AnythingLLM
====================

AnythingLLM is a full-stack application where you can use 
commercial off-the-shelf LLMs or popular open source LLMs
and vectorDB solutions to build a private ChatGPT with no
compromises that you can run locally as well as host remotely
and be able to chat intelligently with any documents you 
provide it.

AnythingLLM divides your documents into objects called 
workspaces. A Workspace functions a lot like a thread, 
but with the addition of containerization of your documents.
Workspaces can share documents, but they do not talk to each 
other so you can keep your context for each workspace clean.




######################################################
############### AA. INSTALL anythingLLM ###############
######################################################


# 0.0 Install Anaconda, if not installed

# 0.1 Syntax to remove conda env -- myenv

        conda remove --name langchain --all

# 0.2 Install on ubuntu plocate to use 'locate'
#     command to search files and net-tools for ifconfig:

sudo apt install plocate
sudo apt install net-tools


# 0.3 For Windows 10/11:
#     Install Microsoft Visual Studio from [this link](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019)
#     While installing you will get a number of options. Under WOrkloads tab, select 'C++ Buildtools' or something like this.
#     This will install nmake utility.
#     Refer StackOverflow: https://stackoverflow.com/a/54136652


# 0.4 For Windows 10/11
#     Next you need to put the following 'bin' folder in the PATH, as: 
#     C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.39.33519\bin\Hostx64\x64
#    USe Advanced System Settings-->Environment Variables-->Path

# 1. Create environment and 
#     setup necessary packages:
#      Open Anaocnda prompt as 'Administrator'
#       and run the following commands, step-by-step:
#        privatGPT requiires python=3.11

# 1.0.1 Add a download channel:
conda config --add channels conda-forge


# 1.0.2 Create conda environment:
conda create --name langchain python=3.11
conda activate langchain
conda install -y spyder jupyter jupyterlab pandas numpy pip-tools ipython
conda install -y langchain -c conda-forge
conda install -y anaconda::beautifulsoup4
conda install -y conda-forge::pypdf
conda install -y conda-forge::transformers
conda install -y fastai::accelerate
conda install -y -c conda-forge huggingface_hub
conda install -y -c anaconda git
conda install -y -c huggingface -c conda-forge datasets
conda install -y conda-forge::openai
conda install -y conda-forge::streamlit
pip install llama-cpp-python
pip install tensorflow
pip install ollama
pip install chromadb
pip install langchain-experimental

######################################################
############### AA.2 INSTALL CUDA ####################
############# AND Appropriate driver #################
######################################################

# 2.0 Install CUDA as in this video. Installation depends upon
#	which GPU is available. For GPU 1060 or 730 install
#	 CUDA 11.4


# 2.0.1 See this link for guidance
# https://github.com/harnalashok/LLMs/blob/main/gpu_nvidia.ipynb
#


######################################################
############### AA.3 Remaining installations #########
######################################################


# 3.0.1 Install git, as:
sudo apt update
sudo apt-get install git
# OR as:
(See https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
sudo apt update
sudo apt install git-all

# 3.0.2 Install curl:
sudo apt update
sudo apt install curl


# 3.0.3 Install 'make':
#	'make' generally comes installed with ubuntu 22.04
#	IF not, install it as:

sudo apt-get install build-essential

######################################################
############### AA.4 INSTALL ollama ##################
######################################################
# 4.0 Install/Uninstall Ollama by following instructions here:
#     See https://github.com/ollama/ollama/blob/main/docs/linux.md:

# 4.0.1 Install as::

curl -fsSL https://ollama.com/install.sh | sh

#  Install the models to be used, the default settings-ollama.yaml
#  is configured to user mistral 7b LLM (~4GB) and nomic-embed-text Embeddings (~275MB).
#  In privateGPT, ollama configuration file is: ~/privateGPT/settings-ollama.yaml

# 4.0.2 Issue following commands to download mistral and nomic-embed-text:
#        Downloaded models are saved to folder /usr/share/ollama/.ollama/models

ollama pull mistral
ollama pull nomic-embed-text

# 4.0.2.1 You can also specify the number of parameters.
#	   Pull ollama:13b model, as:

ollama pull llama2:13b

# And run it as:

ollama run llama2:13b

# 4.0.2.2 Just entering the following command will pull a 7b model and then run
#         but not the already existing 13b model:

ollama run llama2


# 4.0.3 You can test ollama, as:

ollama run mistral

# 4.0.4 On terminal, write a question to get an answer.
#       Exit by typing /bye

# 4.0.5 ollama supports GPUs with compute capability of 5.


# 4.0.6 List all models downloaded:

ollama list


# 4.0.7 Just enter 'ollama' to get ollama help.

Available Commands:
  serve       Start ollama AND also to know ollama port		<===
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models    					<====
  cp          Copy a model
  rm          Remove a model
  help        Help about any command




# 5.0    On reboot ollama starts by itself. The following command is NOT NEEEDED:
#	 It is also very difficult to kill olama process (even by kill -9). After
#	 killing another ollama process starts.

#        ollama serve  # Also informs ollama port: 11434


# 5.0.1 ollama service can be started/restarted/shut, as:
#	Check as: ps aux | grep ollama

sudo systemctl start/stop ollama

# 5.0.2 View ollama logs as:

	journalctl -u ollama  -e


# 5.0.1 On restart, issue again the following commands:

	a. conda activate langchain


# 5.0.2 Install tensorflow:

pip install tensorflow


######################################################
############### BB. anythingLLM ######################
######################################################


# 6.0 Download anythingLLM from here:
#     https://useanything.com/download


# 6.1 It is an App Image. Make it executable, as:

	chmod +x AnythingLLMDesktop.AppImage
	
# 6.1.1 Then execute it, as:

	./AnythingLLMDesktop.AppImage

# 6.2  And begin configuring it as:	
	
#	a. Click Get Started
#	b. Select Ollama as LLM Preference
#	c. Write Ollama base URL, as:  http://localhost:11434
#		Just writing 'localhost:11434' would not work. http://
#			has to be written also.
#	d. Press Tab and under Chat model selection,
#	     you should see a list of models installed. Select one of them
#	f. Under Token Context Window, set token size of 4096.
	   (By default, Ollama uses a context window size of 2048 tokens.
	    It can be change: See this link of FAQ under ollama/docs/faq.md:
	    https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size )
#	g. In the next screen, select 'AnythingLLMEmbdder'
#	h. In the next screen select LanceDB
#		(see below for using chromadb)
#	i  Click Next again
#	j. Select 'For education', click Next
#	h. Give a Workspace name. AnythingLLM can have
#	i.  multiple workspaces for difft document types.
#	Done


# 6.2.1 USing chromadb vector database:
#       Ref: https://docs.trychroma.com/usage-guide#running-chroma-in-clientserver-mode
#
#	Start chromadb in client/server mode, as:
#		In Linux:
#			$conda activate langchain
#			$chroma run --path /home/ashok/Documents/chroma
#		In Windows:
#			>conda activate langchain
#			>chroma run --path C:\Users\ashok\OneDrive\Documents\chroma
#		Open browser and check:
#			http://localhost:8000
#		You should also have an sqlite3 database in chroma folder.
#		In anythingLLM, just write the chroma URL, as:
#			http://localhost:8000 and leav other two fields blank
#			and save changes. No need to specify API key or token.
#

# 6.2.2 Have a look at these youtube videos:
#		1. https://www.youtube.com/watch?v=IJYC6zf86lU
#		2. https://www.youtube.com/watch?v=NuZ0n0LPZ5E&t=83s


# 6.3 Configuration data of AppImage in Linux is within
#	.config folder. Delete from within it 
#	the folder: 'anythingllm-desktopit'
#	 to restore AppImage's original state. 	

# 6.3a In Windows, you have to first uninstall anythingLLM and
#	then delete anythingLLM related files under:
#
#		C:\Users\ashok\AppData\Local\Programs
#   		C:\Users\ashok\AppData\Roaming\anythingllm-desktop\storage
#
#	And then reinstall anythingLLM
	

	    
# 6,4 Ollama on network. Suppose AnythingLLM app image is
#     on a remote machine, then how to access ollama?

	(By default, ollama runs only on localhost. To make
	it accesible on network, access its configuration file:
	sudo nano /etc/systemd/system/ollama.service
	ANd add a line under [Service]
	[Service]
	Environment="OLLAMA_HOST=0.0.0.0"
	Restart ollama, as:
	systemctl daemon-reload
	systemctl restart ollama
	It would now be available on network, ie accsible, as:
	Write Ollama base URL, as:  http://192.240.1.51:11434



######################################
########## DD. NVIDIA driver ##########
############### Problems ##############
#######################################

# 7.0
	It is possible that after privateGPT installs CUDA
	Or you have installed CUDA, then mutiple NVIDIA
	drivers (not one) could be installed. It is also
	possible that when you reboot Ubuntu, screen resolution
	changes rather increases ao that everything is very small.
	Then, in Ubuntu search and open Software & Updates--> Resources (tab)
	Select that minimum driver for your purposes. For example, for CUDA 11.4
	driver version 470 is OK. Higher versions, even though available,
	create problems. It is trial-and-error. Select a driver and 
	reboot to see if it is OK.
	
	Min driver needed?
		See this link for CUDA version and min driver needed:
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html
		OR,
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html#minor-version-compatibility

#####################################
########## EE. Ingestion ############
################ AND ################
############# GPU Usage #############
#####################################


#8.1 Upload a big pdf file:
#			Run in a separate terminal command:
#
#				 nvidia-smi
#
# 			Under GPU-Util one finds GPU utilization
#			increasing from 0% to 33% or some other percentage.

# 8.2 Multiple pdf files upload:
#			Multiple pdf files can be uploaded at the same time.
			But GPU usage increases.
#

# 8.3 Screenshots of nvidia-smi commands:

				
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   47C    P5    13W /  N/A |    817MiB /  6044MiB |     33%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama              61MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                483MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+

## 30%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:17:50 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   62C    P2    49W /  N/A |   1277MiB /  6044MiB |     30%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                521MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


## 52%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:18:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   64C    P2    58W /  N/A |   1279MiB /  6044MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                523MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


------------------------
System Prompt Examples:
-------------------------

	The system prompt can effectively provide your chat bot specialized roles,
	and results tailored to the prompt you have given the model. Examples of 
	system prompts can be be found here: ChatGPT-3.5 Roles:
	https://www.w3schools.com/gen_ai/chatgpt-3-5/chatgpt-3-5_roles.php

	Some interesting examples to try include:

    		You are -X-. You have all the knowledge and personality of -X-. Answer
    		as if you were -X- using their manner of speaking and vocabulary.
        
        	Example: You are Shakespeare. You have all the knowledge and personality
        		 of Shakespeare. Answer as if you were Shakespeare using their manner
        		 of speaking and vocabulary.

			You are an expert (at) -role-. Answer all questions using your expertise
			on -specific domain topic-.

        	Example: You are an expert software engineer. Answer all questions using your 
        		 expertise on Python.

			You are a -role- bot, respond with -response criteria needed-. If 
			no -response criteria- is needed, respond with -alternate response-.
			
        	Example: You are a grammar checking bot, respond with any grammatical corrections
        		 needed. If no corrections are needed, respond with “verified”.
        		 
 Ollama notes:
# 1. Ollama on localhost:
# IF you start ollama as: ollama serve
#    models are stored within /home/ashok/.ollama folder
# IF you start ollama as: sudo systemctl start ollama:
#    downloaded models are saved to: /usr/share/ollama/.ollama/models
# IF ollama is installed as docker service,
#    downloaded models are stored at different place
# 2. Ollama on start binds itself to 127.0.0.1:11434 and NOT
#    at any other IP. But if anyother app on docker is to access
#    ollama, then that app generally creates a bridge within the
#    container at 172.17.0.1 and Ollama must also bind itself to
#    172.17.0.1. To make it bind to this port, see my detailed 
#    reply at this link: https://github.com/khoj-ai/khoj/issues/1100#issuecomment-3245404212.
#    Briefly, start ollama only as: ollama serve. But before that export:
#       $ export OLLAMA_HOST="http://0.0.0.0:11434"   
#       $ ollama serve
#       Now open docker and issue curl command, as:
#      docker exec -it ragflow-server /bin/bash
#      root@a194a08af435:/ragflow#  curl 172.17.0.1:11434
#      It should show connection.
#      Access ollama as: host.docker.internal:11434 where required
        		 


############ DONE #################







