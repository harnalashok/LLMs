{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7320f16b",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Llama 2 Chat\n",
    "https://python.langchain.com/docs/integrations/chat/llama2_chat/\n",
    "https://github.com/dataprofessor/llama2/blob/master/streamlit_app.py\n",
    "https://streamlit.io/gallery?category=llms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1faf2",
   "metadata": {},
   "source": [
    "# Llama2Chat\n",
    "\n",
    "This notebook shows how to augment Llama-2 `LLM`s with the `Llama2Chat` wrapper to support the [Llama-2 chat prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2). Several `LLM` implementations in LangChain can be used as interface to Llama-2 chat models. These include [ChatHuggingFace](/docs/integrations/chat/huggingface), [LlamaCpp](/docs/use_cases/question_answering/local_retrieval_qa), [GPT4All](/docs/integrations/llms/gpt4all), ..., to mention a few examples. \n",
    "\n",
    "`Llama2Chat` is a generic wrapper that implements `BaseChatModel` and can therefore be used in applications as [chat model](/docs/modules/model_io/chat/). `Llama2Chat` converts a list of Messages into the [required chat prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) and forwards the formatted prompt as `str` to the wrapped `LLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c03540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76910f",
   "metadata": {},
   "source": [
    "For the chat application examples below, we'll use the following chat `prompt_template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bbfaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238095fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a297e09",
   "metadata": {},
   "source": [
    "## Chat with Llama-2 via `LlamaCPP` LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1a0b9",
   "metadata": {},
   "source": [
    "For using a Llama-2 chat model with a [LlamaCPP](/docs/integrations/llms/llamacpp) `LMM`, install the `llama-cpp-python` library using [these installation instructions](/docs/integrations/llms/llamacpp#installation). The following example uses a quantized [llama-2-7b-chat.Q4_0.gguf](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf) model stored locally at `~/Models/llama-2-7b-chat.Q4_0.gguf`. You can download it from  [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_0.gguf)\n",
    "\n",
    "After creating a `LlamaCpp` instance, the `llm` is again wrapped into `Llama2Chat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d10bc3-ede6-4410-a867-7c623a0efdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ashok/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.41 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "model_path = \"/home/ashok/Models/llama-2-7b-chat.Q4_0.gguf\"\n",
    "llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                streaming=False,\n",
    "                )\n",
    "\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50498d96",
   "metadata": {},
   "source": [
    "and used in the same way as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90782b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2160b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     783.31 ms\n",
      "llama_print_timings:      sample time =      46.65 ms /    96 runs   (    0.49 ms per token,  2057.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9389.07 ms /   125 tokens (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:        eval time =   15684.02 ms /    95 runs   (  165.09 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time =   25391.57 ms /   220 tokens\n"
     ]
    }
   ],
   "source": [
    "msg=    chain.invoke(\n",
    "                  \"What can I see in Vienna? Propose a few locations. Names only, no details.\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7215809c-e9b2-49c3-a550-8770c291eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here are some popular tourist attractions and landmarks in Vienna:\n",
      "1. Sch√∂nbrunn Palace\n",
      "2. St. Stephen's Cathedral\n",
      "3. Hofburg Palace\n",
      "4. Belvedere Palace\n",
      "5. Prater Amusement Park\n",
      "6. Vienna State Opera\n",
      "7. MuseumsQuartier\n",
      "8. Ringstrasse\n",
      "9. St. Charles Borromeo Church\n",
      "10. Imperial Burial Chapel\n"
     ]
    }
   ],
   "source": [
    "print(msg['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ce06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     783.31 ms\n",
      "llama_print_timings:      sample time =     112.97 ms /   228 runs   (    0.50 ms per token,  2018.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8905.29 ms /   112 tokens (   79.51 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:        eval time =   38896.00 ms /   227 runs   (  171.35 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   48545.35 ms /   339 tokens\n"
     ]
    }
   ],
   "source": [
    "msg1 = chain.invoke(\"Tell me more about #2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea17099-749e-4343-84ba-52515e9dbe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! St. Stephen's Cathedral is a beautiful and historic church located in the heart of Vienna, Austria. It is one of the most famous and recognizable landmarks in the city and is known for its stunning Gothic architecture.\n",
      "The cathedral was built in the 12th century and has undergone several renovations and expansions over the centuries. It features a striking mix of Gothic and Baroque styles, with intricate carvings, colorful stained-glass windows, and a towering spire that offers panoramic views of the city.\n",
      "Inside the cathedral, visitors can admire the ornate decorations, including the tomb of St. Stephen, the patron saint of Vienna, as well as works of art by famous artists such as Hans Holbein the Younger and Lucas Cranach the Elder. The cathedral's organ is also worth mentioning, as it is one of the largest and most powerful in Europe, with over 17,000 pipes.\n",
      "St. Stephen'\n"
     ]
    }
   ],
   "source": [
    "print(msg1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164814c7-d7ed-48ad-9977-98d244a17546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
