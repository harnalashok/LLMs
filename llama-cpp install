#  8th Jan, 2025
# Ref: https://github.com/ggerganov/llama.cpp
#     https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md
# It is possible that Symantic antivirus
#     will not allow langflow start. Then
#     Press win+R and run the following command:
      
        smc -stop


# 1. Open Powershell as Administrator,
#      and issue command:
   wsl --install Ubuntu-22.04
# 2. Reboot machine after installation
# 3. Pin Ubuntu to task-bar and Open Ubuntu
#      allow installation to complete.
# 4. Keep username as ashok also password as ashok.
#     Also Change machine name to either master or ashok
# 5. Reboot machine
# 6. Perform the following install operations:
       sudo apt update
       sudo apt upgrade
       sudo apt install net-tools cmake build-essential -y
# 7. Issue the following commands to build llama.cpp:
#    https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md
  
      git clone https://github.com/ggerganov/llama.cpp
      cd llama.cpp
      cmake -B build
      cmake --build build --config Release
      cd ~
      echo "PATH=\$PATH:/home/ashok/llama.cpp/build/bin" >> .bashrc
      cat .bashrc
  
 # 8. Close and open Ubuntu to take PATH statement into effect.
 # 9. Download a gguf trending model, llama-thinker-3b-preview-q8_0.gguf, from huggingface
 #    into models folder (model size is around 3.2gb): 

       cd /home/ashok/llama.cpp/models
       wget -c   https://huggingface.co/prithivMLmods/Llama-Thinker-3B-Preview-GGUF/resolve/main/llama-thinker-3b-preview-q8_0.gguf?download=true
       # You may have to issue the following command to cleanup also.
       mv 'llama-thinker-3b-preview-q8_0.gguf?download=true' llama-thinker-3b-preview-q8_0.gguf

# 10. Test your installation of llama-cli:
# Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-cli

      cd ~
      llama-cli -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf -p "I believe the meaning of life is" -n 128

# 11. Start llama-server. It will be accessible at port localhost:8080.
#     -a (--alias) flag renames the model to gpt-3.5-turbo:
#    Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-server

   cd ~
   llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080

# 12. Add aditional flags for 4 concrrent users upto 4096 max context (each):

  llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080 -c 16384 -np 4

# 13. Install langflow and access it:

      # 13.1 First install uv
      # Ref: https://docs.astral.sh/uv/getting-started/installation/#installation-methods
         $ curl -LsSf https://astral.sh/uv/install.sh | sh

      # 13.2 Close and open Ubuntu terminal:

      # 13.3 Install langflow
      # Ref: https://docs.langflow.org/get-started-installation

         $ uv venv
         $ uv pip install langflow
      
      # 13.4 Run langflow:

         $ uv run langflow run
      
         # 13,5 Acccess it, at:
         http://127.0.0.1:7860 
      
      # 13.5 Implement Basic Prompting
      #      In the OpenAI widget through 'Controls':
      #      Set URL (OpenAI API Base) to localhost:8080
      #       and model to gpt-3.5-turbo



