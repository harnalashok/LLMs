#  9th Jan, 2025
# Ref: https://github.com/ggerganov/llama.cpp
#     https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md
# It is possible that Symantic antivirus
#     will not allow langflow start. Then
#     Press win+R and run the following command:
      
        smc -stop


########################################################
# Uninstall wsl from Windows 10
########################################################

# 1.0 Open Ubuntu terminal and check existing Ubuntu version:

      lsb_release -a
      
      # If it is 'Ubuntu 22.04.x LTS' then your Ubuntu is   Ubuntu-22.04
      # If it is 'Ubuntu 20.04.x LTS', then your version is Ubuntu-20.04

# 1. Click Windows + I to open Settings
# 2. Then, click Apps --> Apps & features
# 3. Next, in Apps and Features, look for Ubuntu 
     and uninstall it.

# 4. Open Contol Panel-->Programs-->(Under Programs and Features) Turn Windows Features on or off
     In the Window that appears, uncheck 'Windows Subsystem for Linux and click OK.
# 5. Open Powershell as Administrator and issue command:
         wsl --unregister <Your Ubuntu Version>
    # For example:
         wsl --unregister Ubuntu-22.
        (You may get a message:
           There is no distribution with the supplied name.)

 # 6. Also ,in the same Powershell console, issue command:
         wsl --unregister ubuntu

########################################################
# Install wsl Ubuntu
########################################################

# 1. Reboot machine. 
#    Open Powershell as Administrator,
#      and issue command:

   wsl --install Ubuntu-22.04

# 2. Restart machine after installation
# 3. From Start Menu, Pin Ubuntu to task-bar. 
#    Open Ubuntu & allow the installation process to complete.
# 4. When asked, supply username as ashok also password as ashok.
# 5. Perform the following four install/upgrade operations to
     install uv and also ollama:
     (you may have to stop antivirus as: Win+R, and 
       execute the command: smc -stop)

       sudo apt update
       sudo apt upgrade
       sudo apt install net-tools cmake build-essential -y
       curl -LsSf https://astral.sh/uv/install.sh | sh
       curl -fsSL https://ollama.com/install.sh | sh

Script
     echo '[boot]' | sudo tee  /etc/wsl.conf > /dev/null
      echo 'systemd=true' | sudo tee -a /etc/wsl.conf > /dev/null
      echo '[network]' | sudo tee -a /etc/wsl.conf > /dev/null
      echo 'hostname = master' | sudo tee -a /etc/wsl.conf > /dev/null
      echo 'generateHosts = false' | sudo tee -a /etc/wsl.conf > /dev/null
      sudo rm /etc/resolv.conf
      sudo echo "nameserver 8.8.8.8" >> /etc/resolv.conf
      sudo sed -i 's/127.0.1.1.*/127.0.1.1  master.fsm.ac.in   master/' /etc/hosts




##############
# Changing host name
##############
# Ref: https://medium.com/@bonguides25/how-to-change-hostname-of-the-ubuntu-instance-in-wsl-93746998ed26


# 1.0 Reboot Ubuntu.
#      Open /etc/wsl.conf OR 
#      create the file if it did not exist
#       Use the nano text editor.

      sudo nano /etc/wsl.conf


# 2.0 Write the following lines in /etc/wsl.conf
#      And save the file (^O and ^X)

      [boot]
      systemd=true
      [network]
      hostname = master
      generateHosts = false

# 3.0 Shut down WSL using Command Prompt: 

           wsl --shutdown

# 4.0 Open Ubuntu again, Delete the default /etc/resolv.conf file.

	sudo rm /etc/resolv.conf

# 4.1 Create a new /etc/resolv.conf with the following entry.

        sudo echo "nameserver 8.8.8.8" >> /etc/resolv.conf


# 5.0 Also change entries in /etc/hosts file.
#     Replace the present host name, wherever it occurs,
#     with the new name, master.

# 6.0 Reboot Windows

##############
# Build llama.cpp
##############

# 1. Issue the following commands to build llama.cpp as normal user:
#    https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md
  
      git clone https://github.com/ggerganov/llama.cpp
      cd llama.cpp
      cmake -B build
      cmake --build build --config Release
      cd ~
      echo "PATH=\$PATH:/home/ashok/llama.cpp/build/bin" >> .bashrc
      cat .bashrc
  
 # 2. Close and open Ubuntu to take PATH statement into effect.
 # 3. Download a gguf trending model, llama-thinker-3b-preview-q8_0.gguf, from huggingface
 #    into models folder (model size is around 3.2gb): 

       cd /home/ashok/llama.cpp/models
       wget -c   https://huggingface.co/prithivMLmods/Llama-Thinker-3B-Preview-GGUF/resolve/main/llama-thinker-3b-preview-q8_0.gguf?download=true
       # You may have to issue the following command to cleanup also.
       mv 'llama-thinker-3b-preview-q8_0.gguf?download=true' llama-thinker-3b-preview-q8_0.gguf

# 4. Test your installation of llama-cli:
# Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-cli

      cd ~
      llama-cli -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf -p "I believe the meaning of life is" -n 128

# 5. Start llama-server. It will be accessible at port localhost:8080.
#     -a (--alias) flag renames the model to gpt-3.5-turbo:
#    Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-server

   cd ~
   llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080

# 6. Add aditional flags for 4 concrrent users upto 4096 max context (each):

  llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080 -c 16384 -np 4

##############
# Install langflow
##############

# 1. Install langflow and access it:

            # 1.1 First, install uv
            #      Ref: https://docs.astral.sh/uv/getting-started/installation/#installation-methods

                 curl -LsSf https://astral.sh/uv/install.sh | sh
      
            # 1.2 Close and open Ubuntu terminal:
      
            # 1.3 Next, install langflow
            #      Ref: https://docs.langflow.org/get-started-installation
      
                uv venv
                uv pip install langflow
            
            # 1.4 Run langflow:
      
                uv run langflow run
            
               # 1,5 Acccess it, at:
               
                      http://127.0.0.1:7860 
            
            # 1.6 Implement Basic Prompting example:
            #      In the OpenAI widget through 'Controls':
            #        Set OpenAI API Base to 
            #           http://localhost:8080 
            #            (AND NOT just localhost:8080)
            #         and Model Name to gpt-3.5-turbo
            #         and leave API key blank 
            #     Switch to Playground and send a message
            #     to get response.



