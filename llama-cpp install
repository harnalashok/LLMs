#  8th Jan, 2025
# Ref: https://github.com/ggerganov/llama.cpp
#     https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md


# 1. Open Powershell as Administrator,
#      and issue command:
   wsl --install Ubuntu-22.04
# 2. Reboot machine after installation
# 3. Pin Ubuntu to task-bar and Open Ubuntu
#      allow installation to complete.
# 4. Keep username as ashok also password as ashok.
#     Also Change machine name to either master or ashok
# 5. Reboot machine
# 6. Perform the following install operations:
       sudo apt update
       sudo apt upgrade
       sudo apt install net-tools cmake build-essential -y
# 7. Issue the following commands to build llama.cpp:
#    https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md
  
      git clone https://github.com/ggerganov/llama.cpp
      cd llama.cpp
      cmake -B build
      cmake --build build --config Release
      cd ~
      echo "PATH=\$PATH:/home/ashok/llama.cpp/build/bin" >> .bashrc
      cat .bashrc
  
 # 8. Close and open Ubuntu to take PATH statement into effect.
 # 9. Download a gguf trending model, llama-thinker-3b-preview-q8_0.gguf, from huggingface
 #    into models folder (model size is around 3.2gb): 

       cd /home/ashok/llama.cpp/models
       wget -c   https://huggingface.co/prithivMLmods/Llama-Thinker-3B-Preview-GGUF/resolve/main/llama-thinker-3b-preview-q8_0.gguf?download=true
       # You may have to issue the following command to cleanup also.
       mv 'llama-thinker-3b-preview-q8_0.gguf?download=true' llama-thinker-3b-preview-q8_0.gguf

# 10. Test your installation of llama-cli:
# Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-cli

      cd ~
      llama-cli -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf -p "I believe the meaning of life is" -n 128

# 11. Start llama-server. It will be accessible at port localhost:8080.
#     -a (--alias) flag renames the model to gpt-3.5-turbo:
#    Ref: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#llama-server

   cd ~
   llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080
# Add aditional flags for 4 concrrent users upto 4096 max context (each):

  llama-server -a gpt-3.5-turbo -m /home/ashok/llama.cpp/models/llama-thinker-3b-preview-q8_0.gguf --port 8080 -c 16384 -np 4

  
  # Install bare localai without any models
sudo curl https://localai.io/install.sh | sh

# Start/stop local-ai
netstat -aunt | grep 8080
sudo systemctl stop local-ai.service
sudo systemctl disable local-ai.service
sudo systemctl start local-ai.service
netstat -aunt | grep 8080


# localai can also be started, as:
 sudo local-ai

# Ref: https://semaphoreci.com/blog/localai
#      https://localai.io/models/
LocalAI also supports a feature called model gallery. 
You can define language models you want to support by 
setting the PRELOAD_MODELS environment variable. For 
example, the following export replaces gpt-3.5-turbo 
with the GPT4ALL basic model:

# As root run:
# https://localai.io/advanced/#preloading-models-during-startup
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt4all-j"}]' local-ai
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt-3.5-turbo"}]' local-ai
# Trying
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","model": "gpt4all-j","name": "gpt-3.5-turbo"}]' local-ai
# This works as root. File downloads behind the scenes.
# While downloading you should also be able to rename it with:
#   "model": "luna-ai-llama2",
#  "url": "<MODEL_CONFIG_FILE>",
#     "name": "<MODEL_NAME>"
#    https://localai.io/models/#how-to-install-a-model-not-part-of-a-gallery
# Try: luna-ai-llama2
# See:  https://localai.io/advanced/

LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "config_url": "https://raw.githubusercontent.com/mudler/LocalAI/master/embedded/models/hermes-2-pro-mistral.yaml"
   }'

# After downloading this works:
curl http://localhost:8080/v1/chat/completions -H "Content-Type:
  application/json" -d '{ "model": "hermes-2-pro-mistral", "messages": [{"role":
  "user", "content": "How are you doing?", "temperature": 0.1}] }'




LocalAI will advertise the module name, letting you replace
OpenAI models with any model you want. When we start LocalAI
with this variable defined, the API server will automatically
download and cache the model file. 


