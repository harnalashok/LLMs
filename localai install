#  6th Jan, 2025
# Ref: https://localai.io/
#      https://github.com/mudler/LocalAI-examples/tree/main/chatbot-ui

# wsl --install Ubuntu-22.04
# sudo apt update
# sudo apt upgrade
# sudo apt install net-tools
# Install bare localai without any models
sudo curl https://localai.io/install.sh | sh

# Start/stop local-ai
netstat -aunt | grep 8080
sudo systemctl stop local-ai.service
sudo systemctl disable local-ai.service
sudo systemctl start local-ai.service
netstat -aunt | grep 8080


# localai can also be started, as:
 sudo local-ai

# Ref: https://semaphoreci.com/blog/localai
#      https://localai.io/models/
LocalAI also supports a feature called model gallery. 
You can define language models you want to support by 
setting the PRELOAD_MODELS environment variable. For 
example, the following export replaces gpt-3.5-turbo 
with the GPT4ALL basic model:

# As root run:
# https://localai.io/advanced/#preloading-models-during-startup
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt4all-j"}]' local-ai
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt-3.5-turbo"}]' local-ai
# Trying
PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","model": "gpt4all-j","name": "gpt-3.5-turbo"}]' local-ai
# This works as root. File downloads behind the scenes.
# While downloading you should also be able to rename it with:
#   "model": "luna-ai-llama2",
#  "url": "<MODEL_CONFIG_FILE>",
#     "name": "<MODEL_NAME>"
#    https://localai.io/models/#how-to-install-a-model-not-part-of-a-gallery
# Try: luna-ai-llama2
# See:  https://localai.io/advanced/

LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "config_url": "https://raw.githubusercontent.com/mudler/LocalAI/master/embedded/models/hermes-2-pro-mistral.yaml"
   }'

# After downloading this works:
curl http://localhost:8080/v1/chat/completions -H "Content-Type:
  application/json" -d '{ "model": "hermes-2-pro-mistral", "messages": [{"role":
  "user", "content": "How are you doing?", "temperature": 0.1}] }'




LocalAI will advertise the module name, letting you replace
OpenAI models with any model you want. When we start LocalAI
with this variable defined, the API server will automatically
download and cache the model file. 



