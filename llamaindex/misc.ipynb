{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b204332-4714-4589-8961-983ba07102e5",
   "metadata": {},
   "source": [
    "## Use existing vector store\n",
    "\n",
    "To use an existing vector store with LlamaIndex, you can create a VectorStoreIndex from the vector store, allowing you to leverage pre-existing embeddings and data for querying. \n",
    "Here's a breakdown of the process:      \n",
    "\n",
    "\n",
    "## 1. Import Necessary Libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927da6c9-2447-444c-82de-a5d15d65cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e815ee3-4b47-49e3-90eb-2a3fbeeae115",
   "metadata": {},
   "source": [
    "##  2. Initialize the Vector Store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e47a9-0556-462f-9c9c-1f63f97f9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your specific vector store initialization\n",
    "# Example using Chroma:\n",
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"your-collection-name\")\n",
    "vector_store = ChromaVectorStore(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d6bc1-f90d-4733-aebb-ed3b9ab1049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f94fd379-a753-4616-b2bc-2dfbcccea400",
   "metadata": {},
   "source": [
    "## 3. Create the VectorStoreIndex:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139d1d9-2be1-4e8c-a9f0-f02f16b27356",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7b40d-f177-4bc8-b64e-9c24ca2c968d",
   "metadata": {},
   "source": [
    "## Connect to external vector stores (with existing embeddings)\n",
    "\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_guide/      \n",
    "Ref: llamaindex how to decouple construction of vector store and then query it\n",
    "\n",
    "If you have already computed embeddings and dumped them into an external vector store (e.g. Pinecone, Chroma), you can use it with LlamaIndex by:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3becf-26ba-486b-8440-3442ba9b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone.Index(\"quickstart\"))\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
