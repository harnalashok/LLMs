# 26th Jan, 2025
# Miscelleaneous help commands
# For python virtual environments, please see:
#   https://github.com/harnalashok/LLMs/blob/main/python%20venv

# 1. If some service is listening on a port:

netstat -aunt | grep 8000

# 2. Which service is listening on a port

sudo lsof -i:8000 -S

# 3. Stop docker portainer container:

docker stop portainer

# 4. Show me running containers:

docker ps

# 5.0 Show me all containers:

docker ps -a

Notable ports:
	a. 11434	ollama
	b. 8000		chromadb
	c. 3000		flowise
	d. 9091 & 19530 milvus
	e. 7860		langflow
	f. 8080		llama.cpp
	g. 8000		llama-cpp-python   (port can be changed)
        h. 8000         docker-pr (part of docker portainer)
        i  9443         portainer; https://127.0.0.1:9443


Start
  ollama	    sudo systemctl start ollama
  flowise	    ./flowise_start.sh
                     ./start_flowise.sh   (for docker)
  langflow           ./langflow_start.sh
                     ./start_langflow.sh  (for docker)
  chromadb           ./start_chroma.sh
  milvus             ./start_milvus
  portainer          ./start_portainer.sh
  llama.cpp server   ./llamacpp_server.sh
  llama_cpp_python   ./llama_cpp_template.sh

Installed software:
	a. Anaconda
	b. conda environment: langchain
	c. langchain and llama-index
	d. ollama
	e. flowise
	f. langflow
	g. llama.cpp
	h. llama-cpp-python
	i  huggingface_hub
	j. docker
	k. chromadb
	l. milvus vectordb
	m. Node.js
	n. Fast Node Manager (fnm)
  

How to use llama.cpp gguf models in LMStudio
===================

# i)  Create a folder under .lmstudio/models: 
mkdir ~/.lmstudio/models/gemma
# ii) Move to your home folder
cd ~/
# iii) Create a softlink of a gguf file in the current folder:
ln -s ~/llama.cpp/models/gemma-2-2b-it.Q6_K.gguf 
# iv) Move this symlink to flder created above:
mv   gemma-2-2b-it.Q6_K.gguf     ~/.lmstudio/models/gemma/
# iii) 



###########################33
