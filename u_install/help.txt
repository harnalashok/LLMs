# 26th Jan, 2025
# Miscelleaneous help commands
# For python virtual environments, please see:
#   https://github.com/harnalashok/LLMs/blob/main/python%20venv

# 1. If some service is listening on a port:

netstat -aunt | grep 8000

# 2. Which service is listening on a port

sudo lsof -i:8000 -S

# 3. Stop docker portainer container:

docker stop portainer

# 4. Show me running containers:

docker ps

# 5.0 Show me all containers:

docker ps -a

Notable ports:
	a. 11434	ollama
	b. 8000		chromadb
	c. 3000		flowise
	d. 9091 & 19530 milvus
	e. 7860		langflow
	f. 8080		llama.cpp
	g. 8000		llama-cpp-python   (port can be changed)
        h. 8000         docker-pr (part of docker portainer)
        i  9443         portainer; https://127.0.0.1:9443


Start
  ollama	    sudo systemctl start ollama
  flowise	    ./flowise_start.sh
                     ./start_flowise.sh   (for docker)
  langflow           ./langflow_start.sh
                     ./start_langflow.sh  (for docker)
  chromadb           ./start_chroma.sh
  milvus             ./start_milvus
  portainer          ./start_portainer.sh
  llama.cpp server   ./llamacpp_server.sh
  llama_cpp_python   ./llama_cpp_template.sh

Installed software:
	a. Anaconda
	b. conda environment: langchain
	c. langchain and llama-index
	d. ollama
	e. flowise
	f. langflow
	g. llama.cpp
	h. llama-cpp-python
	i  huggingface_hub
	j. docker
	k. chromadb
	l. milvus vectordb
	m. Node.js
	n. Fast Node Manager (fnm)
  

How to use llama.cpp gguf models in LMStudio
===================

# i)  Create a folder under .lmstudio/models: 
     mkdir ~/.lmstudio/models/gemma
# ii) Move to your home folder
     cd ~/
# iii) Create a softlink of a gguf file in the current folder:
      cd ~/
      ln -s ~/llama.cpp/models/gemma-2-2b-it.Q6_K.gguf 
# iv) Move this symlink to flder created above:
      mv gemma-2-2b-it.Q6_K.gguf /home/ashok/.lmstudio/models/gemma
#  v) Start LMStudio and change Models directory to: /home/ashok/.lmstudio

Ollama and local gguf file
===================
Ollama can use existing local gguf models. But, first it writes them in its own way.
In doing so it,essesntially, makes a copy of it. Thus, it is as good as 'ollama pull'
command. To use a gguf model, proceed as follows:

OPEN TERMINAL:

# first make sure you're in your home directory

> cd ~

# Make a file called 'Modelfile' and put one line into it per the docs, I use the nano editor, but anything will do

> nano Modelfile

# type in one line that indicates where to pull the gguf file from, such as:

FROM ~/Downloads/miqu-1-70b.q4_k_m.gguf

# of course the path to the gguf needs to match where you have it.

# save the file: in nano that's CTRL-O (writes it to disk), press enter to confirm filename, then CTRL-X to exit the editor.

# now run the ollama command to create the loadable model

> ollama create <your-model-name-here> -f Modelfile 


ls -la /usr/share/ollama/.ollama/models/blobs/
total 15398268
drwxr-xr-x 2 ollama ollama       4096 Feb  2 16:31 .
drwxr-xr-x 4 ollama ollama       4096 Feb  2 08:53 ..
-rw-r--r-- 1 ollama ollama       7712 Feb  2 09:13 sha256-0b4284c1f87029e67654c7953afa16279961632cf73dcfe33374c4c2f298fa35
-rw-r--r-- 1 ollama ollama 5963057248 Feb  2 09:10 sha256-11f274007f093fefeec994a5dbbb33d0733a4feb87f7ab66dcd7c1069fef0068
-rw-r--r-- 1 ollama ollama        269 Feb  2 09:13 sha256-715415638c9c4c0cb2b78783da041b97bd1205f8b9f9494bd7e5a850cb443602
-rw-r--r-- 1 ollama ollama 7865956224 Feb  2 16:31 sha256-7ddfe27f61bf994542c22aca213c46ecbd8a624cca74abff02a7b5a8c18f787f
-rw-r--r-- 1 ollama ollama        266 Feb  2 16:31 sha256-eaf01729e752f8421efdd4151362fd000bcf66f0af7a9117625d2fbbd0a8b921
-rw-r--r-- 1 ollama ollama 1938763584 Feb  2 09:13 sha256-ece5e659647a20a5c28ab9eea1c12a1ad430bc0f2a27021d00ad103b3bf5206f
-rw-r--r-- 1 ollama ollama        572 Feb  2 09:13 sha256-fbd313562bb706ac00f1a18c0aad8398b3c22d5cd78c47ff6f7246c4c3438576
-rw-r--r-- 1 ollama ollama         32 Feb  2 09:13 sha256-fefc914e46e6024467471837a48a24251db2c6f3f58395943da7bf9dc6f70fb6




###########################33
