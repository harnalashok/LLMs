# A config file to use ollama models
#  as if they are OpenAI models
# Refer: https://docs.litellm.ai/docs/providers/ollama
# Start litellm server as: litellm --config /path/to/config.yaml
# See here how to use it in flowise: https://docs.flowiseai.com/integrations/litellm
model_list:
  - model_name: "mistral:latest"             
    litellm_params:
      model: "ollama_chat/mistral:latest"
      api_base: "http://localhost:11434"
